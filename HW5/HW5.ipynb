{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import emoji\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation, ascii_letters\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import regularizers\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 2000\n",
    "MAX_LEN = 30\n",
    "EMB_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer(lang='ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(get_stop_words('ru'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation).union((' ', '«', '»', '—', '–', '“', '”', '…'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyrillic_letters = set([chr(i) for i in range(ord('а'), ord('я') + 1)] +\n",
    "                       [chr(i) for i in range(ord('А'), ord('Я') + 1)] +\n",
    "                       ['ё', 'Ё'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('../data/summer_reviews.xls')\n",
    "data.columns = ['rating', 'content', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = (data.rating > 3).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем опять же учитывать эмоджи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if (set(token).intersection(cyrillic_letters)\n",
    "            or set(token).intersection(set(ascii_letters))\n",
    "            or token in emoji.UNICODE_EMOJI):\n",
    "            result.append(token)\n",
    "    tokens = [token.lower() for token in result if token.lower() not in stop_words and len(token) >  1]\n",
    "    tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data.content.apply(lambda x: preprocess(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_freq = Counter([val for sublist in data.tokens.tolist() for val in sublist]).most_common(MAX_WORDS)\n",
    "tokens_freq = [word for word, _ in tokens_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {v: k for k, v in enumerate(tokens_freq, start=2)}\n",
    "vocab['UNK'] = 1\n",
    "vocab['PAD'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indices(tokens):\n",
    "    return [vocab.get(token, 1) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['indices'] = data.tokens.apply(lambda x: tokens_to_indices(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['indices'].values,\n",
    "                                                    data['target'].values,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, MAX_LEN, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, MAX_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем эмбеддинги внутри сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(MAX_LEN,))\n",
    "embeddings = tf.keras.layers.Embedding(input_dim=len(vocab), output_dim=EMB_SIZE, activity_regularizer=regularizers.l2(1e-6))(inputs)\n",
    "conv_1 = tf.keras.layers.Conv1D(kernel_size=3, filters=EMB_SIZE, strides=1, activation='relu')(embeddings)\n",
    "pool_1 = tf.keras.layers.MaxPooling1D()(conv_1)\n",
    "conv_2 = tf.keras.layers.Conv1D(kernel_size=2, filters=int(EMB_SIZE / 2), strides=1, activation='relu')(pool_1)\n",
    "pool_2 = tf.keras.layers.MaxPooling1D()(conv_2)\n",
    "conv_3 = tf.keras.layers.Conv1D(kernel_size=2, filters=int(EMB_SIZE / 2), strides=1, activation='relu')(pool_2)\n",
    "pool_3 = tf.keras.layers.MaxPooling1D()(conv_3)\n",
    "flat = tf.keras.layers.Flatten()(pool_3)\n",
    "dense_1 = tf.keras.layers.Dense(64, activation='relu', activity_regularizer=regularizers.l2(1e-4))(flat)\n",
    "drop_1 = tf.keras.layers.Dropout(0.2)(dense_1)\n",
    "dense_2 = tf.keras.layers.Dense(32, activation='relu', activity_regularizer=regularizers.l2(1e-4))(drop_1)\n",
    "drop_2 = tf.keras.layers.Dropout(0.2)(dense_2)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18593 samples, validate on 2066 samples\n",
      "Epoch 1/20\n",
      "18593/18593 [==============================] - 1s 69us/sample - loss: 0.1287 - accuracy: 0.9605 - val_loss: 0.2675 - val_accuracy: 0.8993\n",
      "Epoch 2/20\n",
      "18593/18593 [==============================] - 1s 70us/sample - loss: 0.1250 - accuracy: 0.9619 - val_loss: 0.2753 - val_accuracy: 0.8998\n",
      "Epoch 3/20\n",
      "18593/18593 [==============================] - 1s 68us/sample - loss: 0.1199 - accuracy: 0.9631 - val_loss: 0.2869 - val_accuracy: 0.8998\n",
      "Epoch 4/20\n",
      "18593/18593 [==============================] - 1s 68us/sample - loss: 0.1168 - accuracy: 0.9653 - val_loss: 0.2878 - val_accuracy: 0.8988\n",
      "Epoch 5/20\n",
      "18593/18593 [==============================] - 1s 68us/sample - loss: 0.1126 - accuracy: 0.9675 - val_loss: 0.2960 - val_accuracy: 0.8964\n",
      "Epoch 6/20\n",
      "18593/18593 [==============================] - 1s 68us/sample - loss: 0.1102 - accuracy: 0.9687 - val_loss: 0.2982 - val_accuracy: 0.8974\n",
      "Epoch 7/20\n",
      "18593/18593 [==============================] - 1s 68us/sample - loss: 0.1056 - accuracy: 0.9707 - val_loss: 0.3144 - val_accuracy: 0.8959\n",
      "Epoch 8/20\n",
      "18593/18593 [==============================] - 1s 69us/sample - loss: 0.1012 - accuracy: 0.9713 - val_loss: 0.3162 - val_accuracy: 0.8955\n",
      "Epoch 9/20\n",
      "18593/18593 [==============================] - 1s 69us/sample - loss: 0.0985 - accuracy: 0.9722 - val_loss: 0.3300 - val_accuracy: 0.8945\n",
      "Epoch 10/20\n",
      "18593/18593 [==============================] - 1s 68us/sample - loss: 0.0959 - accuracy: 0.9750 - val_loss: 0.3267 - val_accuracy: 0.8964\n",
      "Epoch 11/20\n",
      "18593/18593 [==============================] - 1s 69us/sample - loss: 0.0921 - accuracy: 0.9751 - val_loss: 0.3309 - val_accuracy: 0.8964\n",
      "Epoch 12/20\n",
      "18593/18593 [==============================] - 1s 69us/sample - loss: 0.0890 - accuracy: 0.9764 - val_loss: 0.3410 - val_accuracy: 0.8945\n",
      "Epoch 13/20\n",
      "18593/18593 [==============================] - 1s 68us/sample - loss: 0.0860 - accuracy: 0.9782 - val_loss: 0.3523 - val_accuracy: 0.8896\n",
      "Epoch 14/20\n",
      "18593/18593 [==============================] - 1s 69us/sample - loss: 0.0820 - accuracy: 0.9798 - val_loss: 0.3516 - val_accuracy: 0.8959\n",
      "Epoch 15/20\n",
      "18593/18593 [==============================] - 1s 69us/sample - loss: 0.0793 - accuracy: 0.9801 - val_loss: 0.3625 - val_accuracy: 0.8877\n",
      "Epoch 16/20\n",
      "18593/18593 [==============================] - 1s 68us/sample - loss: 0.0771 - accuracy: 0.9813 - val_loss: 0.3676 - val_accuracy: 0.8916\n",
      "Epoch 17/20\n",
      "18593/18593 [==============================] - 1s 68us/sample - loss: 0.0741 - accuracy: 0.9821 - val_loss: 0.3702 - val_accuracy: 0.8945\n",
      "Epoch 18/20\n",
      "18593/18593 [==============================] - 1s 68us/sample - loss: 0.0717 - accuracy: 0.9836 - val_loss: 0.3829 - val_accuracy: 0.8882\n",
      "Epoch 19/20\n",
      "18593/18593 [==============================] - 1s 69us/sample - loss: 0.0684 - accuracy: 0.9840 - val_loss: 0.3902 - val_accuracy: 0.8906\n",
      "Epoch 20/20\n",
      "18593/18593 [==============================] - 1s 68us/sample - loss: 0.0658 - accuracy: 0.9851 - val_loss: 0.4007 - val_accuracy: 0.8872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd638230d50>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test, y_test),\n",
    "          batch_size=512,\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем эмбеддинги отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(sentences=data.tokens.values, size=EMB_SIZE, min_count=3, window=3, workers=-1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_freq = Counter([val for sublist in data.tokens.tolist() for val in sublist]).most_common()\n",
    "tokens_freq = [word for word, _ in tokens_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {v: k for k, v in enumerate(tokens_freq, start=2)}\n",
    "vocab['UNK'] = 1\n",
    "vocab['PAD'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(vocab, model):\n",
    "\n",
    "    dim = model.vector_size\n",
    "    weights = np.zeros((len(vocab), dim))\n",
    "\n",
    "    for word, i in vocab.items():\n",
    "        if word == 'PAD':\n",
    "            continue\n",
    "        if word == 'UNK':\n",
    "            weights[i] = np.random.normal(0, 2, dim)\n",
    "        try:\n",
    "            weights[i] = model.wv.get_vector(word)\n",
    "        except KeyError:\n",
    "            weights[i] = np.random.normal(0, 2, dim)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(MAX_LEN,))\n",
    "embeddings = tf.keras.layers.Embedding(input_dim=len(vocab),\n",
    "                                       output_dim=EMB_SIZE,\n",
    "                                       weights=[get_weights(vocab, w2v)],\n",
    "                                       trainable=False,\n",
    "                                       activity_regularizer=regularizers.l2(1e-6))(inputs)\n",
    "conv_1 = tf.keras.layers.Conv1D(kernel_size=3, filters=EMB_SIZE, strides=1, activation='relu')(embeddings)\n",
    "pool_1 = tf.keras.layers.MaxPooling1D()(conv_1)\n",
    "conv_2 = tf.keras.layers.Conv1D(kernel_size=2, filters=int(EMB_SIZE / 2), strides=1, activation='relu')(pool_1)\n",
    "pool_2 = tf.keras.layers.MaxPooling1D()(conv_2)\n",
    "conv_3 = tf.keras.layers.Conv1D(kernel_size=2, filters=int(EMB_SIZE / 2), strides=1, activation='relu')(pool_2)\n",
    "pool_3 = tf.keras.layers.MaxPooling1D()(conv_3)\n",
    "flat = tf.keras.layers.Flatten()(pool_3)\n",
    "dense_1 = tf.keras.layers.Dense(64, activation='relu', activity_regularizer=regularizers.l2(1e-4))(flat)\n",
    "drop_1 = tf.keras.layers.Dropout(0.2)(dense_1)\n",
    "dense_2 = tf.keras.layers.Dense(32, activation='relu', activity_regularizer=regularizers.l2(1e-4))(drop_1)\n",
    "drop_2 = tf.keras.layers.Dropout(0.2)(dense_2)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18593 samples, validate on 2066 samples\n",
      "Epoch 1/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.2110 - accuracy: 0.9119 - val_loss: 0.2627 - val_accuracy: 0.8838\n",
      "Epoch 2/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2048 - accuracy: 0.9150 - val_loss: 0.2645 - val_accuracy: 0.8824\n",
      "Epoch 3/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2005 - accuracy: 0.9182 - val_loss: 0.2639 - val_accuracy: 0.8809\n",
      "Epoch 4/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.1926 - accuracy: 0.9222 - val_loss: 0.2665 - val_accuracy: 0.8843\n",
      "Epoch 5/30\n",
      "18593/18593 [==============================] - 1s 58us/sample - loss: 0.1884 - accuracy: 0.9268 - val_loss: 0.2768 - val_accuracy: 0.8838\n",
      "Epoch 6/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.1818 - accuracy: 0.9306 - val_loss: 0.2709 - val_accuracy: 0.8833\n",
      "Epoch 7/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.1782 - accuracy: 0.9316 - val_loss: 0.2826 - val_accuracy: 0.8829\n",
      "Epoch 8/30\n",
      "18593/18593 [==============================] - 1s 55us/sample - loss: 0.1768 - accuracy: 0.9336 - val_loss: 0.2756 - val_accuracy: 0.8848\n",
      "Epoch 9/30\n",
      "18593/18593 [==============================] - 1s 55us/sample - loss: 0.1709 - accuracy: 0.9380 - val_loss: 0.2786 - val_accuracy: 0.8848\n",
      "Epoch 10/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.1656 - accuracy: 0.9395 - val_loss: 0.2779 - val_accuracy: 0.8833\n",
      "Epoch 11/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.1594 - accuracy: 0.9449 - val_loss: 0.2995 - val_accuracy: 0.8833\n",
      "Epoch 12/30\n",
      "18593/18593 [==============================] - 1s 55us/sample - loss: 0.1617 - accuracy: 0.9419 - val_loss: 0.2785 - val_accuracy: 0.8804\n",
      "Epoch 13/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.1493 - accuracy: 0.9479 - val_loss: 0.3168 - val_accuracy: 0.8800\n",
      "Epoch 14/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.1431 - accuracy: 0.9518 - val_loss: 0.3176 - val_accuracy: 0.8771\n",
      "Epoch 15/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.1337 - accuracy: 0.9565 - val_loss: 0.3227 - val_accuracy: 0.8775\n",
      "Epoch 16/30\n",
      "18593/18593 [==============================] - 1s 55us/sample - loss: 0.1328 - accuracy: 0.9561 - val_loss: 0.3407 - val_accuracy: 0.8800\n",
      "Epoch 17/30\n",
      "18593/18593 [==============================] - 1s 55us/sample - loss: 0.1325 - accuracy: 0.9565 - val_loss: 0.3489 - val_accuracy: 0.8766\n",
      "Epoch 18/30\n",
      "18593/18593 [==============================] - 1s 55us/sample - loss: 0.1234 - accuracy: 0.9603 - val_loss: 0.3209 - val_accuracy: 0.8824\n",
      "Epoch 19/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.1204 - accuracy: 0.9624 - val_loss: 0.3409 - val_accuracy: 0.8795\n",
      "Epoch 20/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.1145 - accuracy: 0.9642 - val_loss: 0.3374 - val_accuracy: 0.8809\n",
      "Epoch 21/30\n",
      "18593/18593 [==============================] - 1s 55us/sample - loss: 0.1116 - accuracy: 0.9656 - val_loss: 0.3680 - val_accuracy: 0.8746\n",
      "Epoch 22/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.1039 - accuracy: 0.9693 - val_loss: 0.3508 - val_accuracy: 0.8780\n",
      "Epoch 23/30\n",
      "18593/18593 [==============================] - 1s 55us/sample - loss: 0.1131 - accuracy: 0.9682 - val_loss: 0.4154 - val_accuracy: 0.8703\n",
      "Epoch 24/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.0993 - accuracy: 0.9706 - val_loss: 0.3791 - val_accuracy: 0.8761\n",
      "Epoch 25/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.1027 - accuracy: 0.9676 - val_loss: 0.4198 - val_accuracy: 0.8669\n",
      "Epoch 26/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.1015 - accuracy: 0.9698 - val_loss: 0.4212 - val_accuracy: 0.8693\n",
      "Epoch 27/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.0881 - accuracy: 0.9753 - val_loss: 0.4372 - val_accuracy: 0.8722\n",
      "Epoch 28/30\n",
      "18593/18593 [==============================] - 1s 58us/sample - loss: 0.0870 - accuracy: 0.9747 - val_loss: 0.4830 - val_accuracy: 0.8688\n",
      "Epoch 29/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.0851 - accuracy: 0.9756 - val_loss: 0.4221 - val_accuracy: 0.8683\n",
      "Epoch 30/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.0837 - accuracy: 0.9772 - val_loss: 0.4269 - val_accuracy: 0.8674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd608155950>"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test, y_test),\n",
    "          batch_size=256,\n",
    "          epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе результат примерное такой же или чуть хуже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
