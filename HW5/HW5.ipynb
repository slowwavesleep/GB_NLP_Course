{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import emoji\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation, ascii_letters\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import regularizers\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 2000\n",
    "MAX_LEN = 30\n",
    "EMB_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer(lang='ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(get_stop_words('ru'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation).union((' ', '«', '»', '—', '–', '“', '”', '…'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyrillic_letters = set([chr(i) for i in range(ord('а'), ord('я') + 1)] +\n",
    "                       [chr(i) for i in range(ord('А'), ord('Я') + 1)] +\n",
    "                       ['ё', 'Ё'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('../data/summer_reviews.xls')\n",
    "data.columns = ['rating', 'content', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = (data.rating > 3).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем опять же учитывать эмоджи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if (set(token).intersection(cyrillic_letters)\n",
    "            or set(token).intersection(set(ascii_letters))\n",
    "            or token in emoji.UNICODE_EMOJI):\n",
    "            result.append(token)\n",
    "    tokens = [token.lower() for token in result if token.lower() not in stop_words]\n",
    "    tokens = [token for token in tokens if token in emoji.UNICODE_EMOJI or len(token) >  1]\n",
    "    tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data.content.apply(lambda x: preprocess(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_freq = Counter([val for sublist in data.tokens.tolist() for val in sublist]).most_common(MAX_WORDS)\n",
    "tokens_freq = [word for word, _ in tokens_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {v: k for k, v in enumerate(tokens_freq, start=2)}\n",
    "vocab['UNK'] = 1\n",
    "vocab['PAD'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indices(tokens):\n",
    "    return [vocab.get(token, 1) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['indices'] = data.tokens.apply(lambda x: tokens_to_indices(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['indices'].values,\n",
    "                                                    data['target'].values,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, MAX_LEN, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, MAX_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем эмбеддинги внутри сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(MAX_LEN,))\n",
    "embeddings = tf.keras.layers.Embedding(input_dim=len(vocab), output_dim=EMB_SIZE, activity_regularizer=regularizers.l2(1e-6))(inputs)\n",
    "conv_1 = tf.keras.layers.Conv1D(kernel_size=3, filters=EMB_SIZE, strides=1, activation='relu')(embeddings)\n",
    "pool_1 = tf.keras.layers.MaxPooling1D()(conv_1)\n",
    "conv_2 = tf.keras.layers.Conv1D(kernel_size=2, filters=int(EMB_SIZE / 2), strides=1, activation='relu')(pool_1)\n",
    "pool_2 = tf.keras.layers.MaxPooling1D()(conv_2)\n",
    "conv_3 = tf.keras.layers.Conv1D(kernel_size=2, filters=int(EMB_SIZE / 2), strides=1, activation='relu')(pool_2)\n",
    "pool_3 = tf.keras.layers.MaxPooling1D()(conv_3)\n",
    "flat = tf.keras.layers.Flatten()(pool_3)\n",
    "dense_1 = tf.keras.layers.Dense(64, activation='relu', activity_regularizer=regularizers.l2(1e-4))(flat)\n",
    "drop_1 = tf.keras.layers.Dropout(0.2)(dense_1)\n",
    "dense_2 = tf.keras.layers.Dense(32, activation='relu', activity_regularizer=regularizers.l2(1e-4))(drop_1)\n",
    "drop_2 = tf.keras.layers.Dropout(0.2)(dense_2)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18593 samples, validate on 2066 samples\n",
      "Epoch 1/20\n",
      "18593/18593 [==============================] - 2s 106us/sample - loss: 0.4078 - accuracy: 0.8369 - val_loss: 0.2985 - val_accuracy: 0.8698\n",
      "Epoch 2/20\n",
      "18593/18593 [==============================] - 1s 75us/sample - loss: 0.2400 - accuracy: 0.8991 - val_loss: 0.2343 - val_accuracy: 0.9042\n",
      "Epoch 3/20\n",
      "18593/18593 [==============================] - 1s 76us/sample - loss: 0.1941 - accuracy: 0.9230 - val_loss: 0.2189 - val_accuracy: 0.9071\n",
      "Epoch 4/20\n",
      "18593/18593 [==============================] - 1s 74us/sample - loss: 0.1707 - accuracy: 0.9329 - val_loss: 0.2373 - val_accuracy: 0.9042\n",
      "Epoch 5/20\n",
      "18593/18593 [==============================] - 1s 76us/sample - loss: 0.1506 - accuracy: 0.9430 - val_loss: 0.2586 - val_accuracy: 0.9013\n",
      "Epoch 6/20\n",
      "18593/18593 [==============================] - 1s 76us/sample - loss: 0.1303 - accuracy: 0.9554 - val_loss: 0.2710 - val_accuracy: 0.9042\n",
      "Epoch 7/20\n",
      "18593/18593 [==============================] - 1s 74us/sample - loss: 0.1133 - accuracy: 0.9617 - val_loss: 0.2885 - val_accuracy: 0.8921\n",
      "Epoch 8/20\n",
      "18593/18593 [==============================] - 1s 74us/sample - loss: 0.0932 - accuracy: 0.9707 - val_loss: 0.3732 - val_accuracy: 0.8925\n",
      "Epoch 9/20\n",
      "18593/18593 [==============================] - 1s 76us/sample - loss: 0.0776 - accuracy: 0.9764 - val_loss: 0.4240 - val_accuracy: 0.8892\n",
      "Epoch 10/20\n",
      "18593/18593 [==============================] - 1s 76us/sample - loss: 0.0634 - accuracy: 0.9831 - val_loss: 0.4398 - val_accuracy: 0.8858\n",
      "Epoch 11/20\n",
      "18593/18593 [==============================] - 1s 78us/sample - loss: 0.0565 - accuracy: 0.9845 - val_loss: 0.4769 - val_accuracy: 0.8858\n",
      "Epoch 12/20\n",
      "18593/18593 [==============================] - 1s 75us/sample - loss: 0.0537 - accuracy: 0.9851 - val_loss: 0.4741 - val_accuracy: 0.8935\n",
      "Epoch 13/20\n",
      "18593/18593 [==============================] - 1s 76us/sample - loss: 0.0483 - accuracy: 0.9871 - val_loss: 0.4743 - val_accuracy: 0.8940\n",
      "Epoch 14/20\n",
      "18593/18593 [==============================] - 1s 74us/sample - loss: 0.0439 - accuracy: 0.9884 - val_loss: 0.5335 - val_accuracy: 0.8921\n",
      "Epoch 15/20\n",
      "18593/18593 [==============================] - 1s 76us/sample - loss: 0.0443 - accuracy: 0.9887 - val_loss: 0.5442 - val_accuracy: 0.8892\n",
      "Epoch 16/20\n",
      "18593/18593 [==============================] - 1s 74us/sample - loss: 0.0441 - accuracy: 0.9881 - val_loss: 0.5484 - val_accuracy: 0.8901\n",
      "Epoch 17/20\n",
      "18593/18593 [==============================] - 1s 74us/sample - loss: 0.0422 - accuracy: 0.9889 - val_loss: 0.5790 - val_accuracy: 0.8867\n",
      "Epoch 18/20\n",
      "18593/18593 [==============================] - 1s 74us/sample - loss: 0.0402 - accuracy: 0.9894 - val_loss: 0.5975 - val_accuracy: 0.8882\n",
      "Epoch 19/20\n",
      "18593/18593 [==============================] - 1s 75us/sample - loss: 0.0466 - accuracy: 0.9864 - val_loss: 0.5597 - val_accuracy: 0.8925\n",
      "Epoch 20/20\n",
      "18593/18593 [==============================] - 1s 75us/sample - loss: 0.0411 - accuracy: 0.9890 - val_loss: 0.6065 - val_accuracy: 0.8872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2bbc133890>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test, y_test),\n",
    "          batch_size=256,\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем эмбеддинги отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(sentences=data.tokens.values, size=EMB_SIZE, min_count=3, window=3, workers=-1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_freq = Counter([val for sublist in data.tokens.tolist() for val in sublist]).most_common()\n",
    "tokens_freq = [word for word, _ in tokens_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {v: k for k, v in enumerate(tokens_freq, start=2)}\n",
    "vocab['UNK'] = 1\n",
    "vocab['PAD'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(vocab, model):\n",
    "\n",
    "    dim = model.vector_size\n",
    "    weights = np.zeros((len(vocab), dim))\n",
    "\n",
    "    for word, i in vocab.items():\n",
    "        if word == 'PAD':\n",
    "            continue\n",
    "        if word == 'UNK':\n",
    "            weights[i] = np.random.normal(0, 2, dim)\n",
    "        try:\n",
    "            weights[i] = model.wv.get_vector(word)\n",
    "        except KeyError:\n",
    "            weights[i] = np.random.normal(0, 2, dim)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(MAX_LEN,))\n",
    "embeddings = tf.keras.layers.Embedding(input_dim=len(vocab),\n",
    "                                       output_dim=EMB_SIZE,\n",
    "                                       weights=[get_weights(vocab, w2v)],\n",
    "                                       trainable=False,\n",
    "                                       activity_regularizer=regularizers.l2(1e-6))(inputs)\n",
    "conv_1 = tf.keras.layers.Conv1D(kernel_size=3, filters=EMB_SIZE, strides=1, activation='relu')(embeddings)\n",
    "pool_1 = tf.keras.layers.MaxPooling1D()(conv_1)\n",
    "conv_2 = tf.keras.layers.Conv1D(kernel_size=2, filters=int(EMB_SIZE / 2), strides=1, activation='relu')(pool_1)\n",
    "pool_2 = tf.keras.layers.MaxPooling1D()(conv_2)\n",
    "conv_3 = tf.keras.layers.Conv1D(kernel_size=2, filters=int(EMB_SIZE / 2), strides=1, activation='relu')(pool_2)\n",
    "pool_3 = tf.keras.layers.MaxPooling1D()(conv_3)\n",
    "flat = tf.keras.layers.Flatten()(pool_3)\n",
    "dense_1 = tf.keras.layers.Dense(64, activation='relu', activity_regularizer=regularizers.l2(1e-4))(flat)\n",
    "drop_1 = tf.keras.layers.Dropout(0.2)(dense_1)\n",
    "dense_2 = tf.keras.layers.Dense(32, activation='relu', activity_regularizer=regularizers.l2(1e-4))(drop_1)\n",
    "drop_2 = tf.keras.layers.Dropout(0.2)(dense_2)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18593 samples, validate on 2066 samples\n",
      "Epoch 1/30\n",
      "18593/18593 [==============================] - 2s 85us/sample - loss: 0.5050 - accuracy: 0.8267 - val_loss: 0.4210 - val_accuracy: 0.8335\n",
      "Epoch 2/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.4207 - accuracy: 0.8342 - val_loss: 0.4205 - val_accuracy: 0.8364\n",
      "Epoch 3/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.4171 - accuracy: 0.8367 - val_loss: 0.4184 - val_accuracy: 0.8345\n",
      "Epoch 4/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.4100 - accuracy: 0.8368 - val_loss: 0.4055 - val_accuracy: 0.8349\n",
      "Epoch 5/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.3745 - accuracy: 0.8413 - val_loss: 0.3718 - val_accuracy: 0.8379\n",
      "Epoch 6/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.3474 - accuracy: 0.8505 - val_loss: 0.3554 - val_accuracy: 0.8393\n",
      "Epoch 7/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.3194 - accuracy: 0.8583 - val_loss: 0.3280 - val_accuracy: 0.8548\n",
      "Epoch 8/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.3009 - accuracy: 0.8648 - val_loss: 0.3132 - val_accuracy: 0.8611\n",
      "Epoch 9/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2862 - accuracy: 0.8712 - val_loss: 0.3086 - val_accuracy: 0.8596\n",
      "Epoch 10/30\n",
      "18593/18593 [==============================] - 1s 55us/sample - loss: 0.2767 - accuracy: 0.8759 - val_loss: 0.3166 - val_accuracy: 0.8495\n",
      "Epoch 11/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2725 - accuracy: 0.8776 - val_loss: 0.2952 - val_accuracy: 0.8722\n",
      "Epoch 12/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.2627 - accuracy: 0.8815 - val_loss: 0.2928 - val_accuracy: 0.8742\n",
      "Epoch 13/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.2594 - accuracy: 0.8823 - val_loss: 0.2883 - val_accuracy: 0.8703\n",
      "Epoch 14/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2540 - accuracy: 0.8886 - val_loss: 0.2960 - val_accuracy: 0.8703\n",
      "Epoch 15/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2475 - accuracy: 0.8909 - val_loss: 0.2860 - val_accuracy: 0.8732\n",
      "Epoch 16/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2419 - accuracy: 0.8930 - val_loss: 0.2765 - val_accuracy: 0.8751\n",
      "Epoch 17/30\n",
      "18593/18593 [==============================] - 1s 55us/sample - loss: 0.2358 - accuracy: 0.8989 - val_loss: 0.2851 - val_accuracy: 0.8756\n",
      "Epoch 18/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2359 - accuracy: 0.8974 - val_loss: 0.2805 - val_accuracy: 0.8829\n",
      "Epoch 19/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2277 - accuracy: 0.9027 - val_loss: 0.2705 - val_accuracy: 0.8838\n",
      "Epoch 20/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2218 - accuracy: 0.9061 - val_loss: 0.2705 - val_accuracy: 0.8814\n",
      "Epoch 21/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2191 - accuracy: 0.9086 - val_loss: 0.2733 - val_accuracy: 0.8867\n",
      "Epoch 22/30\n",
      "18593/18593 [==============================] - 1s 55us/sample - loss: 0.2128 - accuracy: 0.9136 - val_loss: 0.2867 - val_accuracy: 0.8751\n",
      "Epoch 23/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2092 - accuracy: 0.9150 - val_loss: 0.2796 - val_accuracy: 0.8751\n",
      "Epoch 24/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.2057 - accuracy: 0.9180 - val_loss: 0.2809 - val_accuracy: 0.8756\n",
      "Epoch 25/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.2004 - accuracy: 0.9199 - val_loss: 0.2752 - val_accuracy: 0.8833\n",
      "Epoch 26/30\n",
      "18593/18593 [==============================] - 1s 56us/sample - loss: 0.1933 - accuracy: 0.9263 - val_loss: 0.2849 - val_accuracy: 0.8775\n",
      "Epoch 27/30\n",
      "18593/18593 [==============================] - 1s 59us/sample - loss: 0.1875 - accuracy: 0.9280 - val_loss: 0.2916 - val_accuracy: 0.8732\n",
      "Epoch 28/30\n",
      "18593/18593 [==============================] - 1s 58us/sample - loss: 0.1894 - accuracy: 0.9283 - val_loss: 0.2926 - val_accuracy: 0.8848\n",
      "Epoch 29/30\n",
      "18593/18593 [==============================] - 1s 58us/sample - loss: 0.1875 - accuracy: 0.9274 - val_loss: 0.2828 - val_accuracy: 0.8703\n",
      "Epoch 30/30\n",
      "18593/18593 [==============================] - 1s 57us/sample - loss: 0.1734 - accuracy: 0.9362 - val_loss: 0.3172 - val_accuracy: 0.8751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2b7f276c50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test, y_test),\n",
    "          batch_size=256,\n",
    "          epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе результат на валидационных данных примерно такой же или чуть хуже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
