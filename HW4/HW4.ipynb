{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë–µ—Ä–µ–º –∑–∞–¥–∞—á–∫—É —Å –æ—Ç–∑—ã–≤–∞–º–∏ –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–µ–º:\n",
    "- —É—á–∏–º –¥–µ—Ä–µ–≤—å—è –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –ª–æ–≥ —Ä–µ–≥–æ–º\n",
    "- –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ —á–∏—Å–ª—É —Å–ª–æ–≤ (–≤–æ–æ–±—â–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –º–æ–∂–µ—Ç –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä - —Å–ª–æ–≤–∞—Ä–∏–∫)\n",
    "- —É—á–∏–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å (–ø–æ–¥–æ–±—Ä–∞—Ç—å —á–∏—Å–ª–æ —Ç–µ–º, –ø–æ–∏–≥—Ä–∞—Ç—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "import emoji\n",
    "from string import ascii_letters, punctuation, digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import pymorphy2\n",
    "import gensim\n",
    "from stop_words import get_stop_words\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('../data/summer_reviews.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['rating', 'content', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä–∏–º –∫–∞–∫–∏–µ –≤–æ–æ–±—â–µ —Å–∏–º–≤–æ–ª—ã –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –æ—Ç–∑—ã–≤–∞—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_symbols = Counter()\n",
    "_ = data.content.apply(lambda x: content_symbols.update(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å, —á—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —ç–º–æ–¥–∂–∏, –Ω–µ–±—É–∫–≤–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, —Å–∏–º–≤–æ–ª—ã —Å –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –∑–Ω–∞–∫–∞–º–∏, –∞ —Ç–∞–∫–∂–µ –∏–µ—Ä–æ–≥–ª–∏—Ñ—ã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('q', 26),\n",
       " ('üëé', 26),\n",
       " ('Q', 25),\n",
       " ('üòâ', 23),\n",
       " ('‚òÜ', 21),\n",
       " ('–Å', 19),\n",
       " ('üòÉ', 19),\n",
       " ('j', 17),\n",
       " ('üëè', 17),\n",
       " ('‚Äî', 16),\n",
       " ('\\u200b', 16),\n",
       " ('üòÑ', 16),\n",
       " ('‚ò∫', 15),\n",
       " ('=', 15),\n",
       " ('B', 15),\n",
       " ('V', 13),\n",
       " ('–©', 13),\n",
       " ('üòÇ', 12),\n",
       " ('J', 11),\n",
       " ('üí™', 11)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_symbols.most_common()[140:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data.content.apply(lambda x: word_tokenize(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_emojis(tokens):\n",
    "    return len([token for token in tokens if token in emoji.UNICODE_EMOJI])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emoji_num'] = data.tokens.apply(lambda x: count_emojis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyrillic_letters = set([chr(i) for i in range(ord('–∞'), ord('—è') + 1)] +\n",
    "                       [chr(i) for i in range(ord('–ê'), ord('–Ø') + 1)] +\n",
    "                       ['—ë', '–Å'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cyrillic(tokens):\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        for ch in token:\n",
    "            if ch in cyrillic_letters:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_latin(tokens):\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        for ch in token:\n",
    "            if ch in ascii_letters:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cyrillic_num'] = data.tokens.apply(lambda x: count_cyrillic(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['latin_num'] = data.tokens.apply(lambda x: count_latin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¥–æ–ø–æ–ª–Ω–∏–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
    "punctuation = set(punctuation).union((' ', '¬´', '¬ª', '‚Äî', '‚Äì', '‚Äú', '‚Äù', '‚Ä¶'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_non_alpha = [sym for sym in content_symbols if (not sym.isalpha()\n",
    "                                                      and sym not in ascii_letters\n",
    "                                                      and sym not in cyrillic_letters\n",
    "                                                      and sym not in punctuation\n",
    "                                                      and sym not in digits\n",
    "                                                      and sym not in emoji.UNICODE_EMOJI)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Å—Ç–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –±—É–¥–µ–º –æ—Ç–±—Ä–∞—Å—ã–≤–∞—Ç—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tokens = data.tokens.apply(lambda x: [token for token in x if token not in other_non_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_other(tokens):\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        for ch in token:\n",
    "            if (ch.isalpha()\n",
    "                and ch not in ascii_letters\n",
    "                and ch not in cyrillic_letters):\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—É–∫–≤ –∏–∑ –¥—Ä—É–≥–∏—Ö –∞–ª—Ñ–∞–≤–∏—Ç–æ–≤, –∫–æ—Å–≤–µ–Ω–Ω–æ —É–∫–∞–∑—ã–≤–∞—é—â–µ–µ –Ω–∞ –¥—Ä—É–≥–æ–π —è–∑—ã–∫\n",
    "data['other_num'] = data.tokens.apply(lambda x: count_other(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokens):\n",
    "    return len([token for token in tokens if token.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uppercase(tokens):\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        for ch in token:\n",
    "            if (ch.isalpha\n",
    "                and ch.isupper()):\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uppercase_words(tokens):\n",
    "    return len([token for token in tokens if token.isalpha() and token.isupper() and len(token) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_num'] = data.tokens.apply(lambda x: count_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['uppercase_num'] = data.tokens.apply(lambda x: count_uppercase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['uppercase_word_num'] = data.tokens.apply(lambda x: count_uppercase_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = (data.rating > 3).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏. –ü–æ–ø—Ä–æ–±—É–µ–º –≤–∫–ª—é—á–∏—Ç—å –≤ —Ç–æ–∫–µ–Ω—ã —Ç–∞–∫–∂–µ –∏ —ç–º–æ–¥–∂–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(token, morph=morph):\n",
    "    return morph.parse(token)[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens(tokens):\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if (token.isalpha()):\n",
    "            result.append(lemmatize(token).lower())\n",
    "        elif token in emoji.UNICODE_EMOJI:\n",
    "            result.append(token)\n",
    "        else:\n",
    "            pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['vec_tokens'] = data.tokens.apply(lambda x: ' '.join(process_tokens(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.999,\n",
       "                max_features=None, min_df=0.001, ngram_range=(1, 1), norm='l2',\n",
       "                preprocessor=None, smooth_idf=True, stop_words=None,\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer(min_df=0.001, max_df=0.999)\n",
    "vec.fit(X_train.vec_tokens.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = vec.transform(X_train.vec_tokens.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vec = vec.transform(X_test.vec_tokens.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_vocab = {v:k for k, v in vec.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = lr.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å—Ç–µ—Ç, –∫–æ–≥–¥–∞ –ø–æ—á—Ç–∏ –Ω–µ —É—Ä–µ–∑–∞–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–∞–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9437268002969562"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ –æ —Ç–æ–º, —á—Ç–æ —ç–º–æ–¥–∂–∏ –±—É–¥—É—Ç –ø–æ–ª–µ–∑–Ω—ã –Ω–µ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–æ—Å—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—É–¥–æ–±–Ω–æ\n",
      "–æ—Ç–ª–∏—á–Ω—ã–π\n",
      "—Å–ø–∞—Å–∏–±–æ\n",
      "—É–¥–æ–±–Ω—ã–π\n",
      "—Å—É–ø–µ—Ä\n",
      "—Ö–æ—Ä–æ—à–æ\n",
      "–æ—Ç–ª–∏—á–Ω–æ\n",
      "–Ω—Ä–∞–≤–∏—Ç—å—Å—è\n",
      "—Ö–æ—Ä–æ—à–∏–π\n",
      "–¥–æ–≤–æ–ª—å–Ω—ã–π\n"
     ]
    }
   ],
   "source": [
    "for i in lr.coef_[0].argsort()[::-1][:10]:\n",
    "    print(vec_vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–Ω–µ\n",
      "–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ\n",
      "—É–∂–∞—Å–Ω–æ\n",
      "—É–∂–∞—Å–Ω—ã–π\n",
      "–∞–Ω—Ç–∏–≤–∏—Ä—É—Å\n",
      "–≤—ã–ª–µ—Ç–∞—Ç—å\n",
      "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π\n",
      "–ø—Ä–æ—à–∏–≤–∫–∞\n",
      "—Ä—É—Ç–∞\n",
      "root\n"
     ]
    }
   ],
   "source": [
    "for i in lr.coef_[0].argsort()[:10]:\n",
    "    print(vec_vocab[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ—Å—Ç–∞–≤–∏–º —Å–ª–æ–≤–∞—Ä–∏ –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö —Å–ª–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dict = set([vec_vocab[i] for i in lr.coef_[0].argsort()[:100]])\n",
    "pos_dict = set([vec_vocab[i] for i in lr.coef_[0].argsort()[::-1][:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = rf.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9385491606714628"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ–∏—á–∏ —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = data.columns.drop(['rating', 'content', 'date', 'tokens', 'target', 'vec_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['emoji_num', 'cyrillic_num', 'latin_num', 'other_num', 'word_num',\n",
       "       'uppercase_num', 'uppercase_word_num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feats = X_train[feat_cols]\n",
    "X_test_feats = X_test[feat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_feats, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = lr.predict(X_test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9060139860139861"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = rf.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.938885560215698"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Ü–µ–ª–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–µ–ø–ª–æ—Ö–æ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ø—Ä–æ–±—É–µ–º –≤—Å–µ –≤–º–µ—Å—Ç–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16527, 768)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = np.hstack([X_train_vec.todense(), X_train_feats])\n",
    "X_test_all = np.hstack([X_test_vec.todense(), X_test_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_all, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = lr.predict(X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94351371386212"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_all, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = rf.predict(X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9399641577060931"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Å–æ–±–æ–π –ø–æ–ª—å–∑—ã —ç—Ç–æ –Ω–µ –¥–∞–µ—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ø—Ä–æ–±—É–µ–º –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å–ª–æ–≤–∞—Ä—è–º–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pos_num'] = data.tokens.apply(lambda x: len([word for word in x if word.lower() in pos_dict]))\n",
    "data['neg_num'] = data.tokens.apply(lambda x: len([word for word in x if word.lower() in neg_dict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dict = X_test[['pos_num', 'neg_num']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∏–º –ø—Ä–∏–º–∏—Ç–∏–≤–Ω—ã–º –æ–±—Ä–∞–∑–æ–º —Ç–æ–∂–µ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–ø–ª–æ—Ö–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7508746087276744"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, X_test_dict.pos_num - X_test_dict.neg_num > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dict = X_train[['pos_num', 'neg_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_dict, y_train)\n",
    "lr_preds = lr.predict(X_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9234980550352975"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_negation(text):\n",
    "    return re.sub(r'([–ù–Ω][–ï–µ]) (\\w)', '\\g<1>_\\g<2>', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lda_tokens'] = data.vec_tokens.apply(lambda x: combine_negation(x))\n",
    "data['lda_tokens'] = data.lda_tokens.apply(lambda x: x.split())\n",
    "data['lda_tokens'] = data.lda_tokens.apply(lambda x: [token for token in x if token not in stop_words and token not in emoji.UNICODE_EMOJI])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3        [—Å—Ç–∞—Ç—å, –∑–∞–≤–∏—Å–∞—Ç—å, —Ä–∞–±–æ—Ç–∞, –∞–Ω—Ç–∏–≤–∏—Ä—É—Å, –¥–∞–ª—ë–∫–∏–π, ...\n",
       "11                       [–Ω–æ—Ä–º–∞–ª—å–Ω–æ, —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ, —É–¥–∞–ª—è—Ç—å]\n",
       "12       [–Ω–µ_—Å—Ç–∞—Ä—Ç–æ–≤–∞—Ç—å, –¥–æ—Å—Ç—É–ø, gps, sms, –∑–≤–æ–Ω–æ–∫, –∞–¥—Ä–µ...\n",
       "13           [—É–¥–æ–±–Ω–æ, —Ä–∞–±–æ—Ç–∞—Ç—å, –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π, –ø–æ–¥–≤–∏—Å–∞–Ω–∏—è]\n",
       "25       [–∑–∞—Ö–æ–¥, –¥–≤–∞–∂–¥—ã, —Ç—Ä–µ–±–æ–≤–∞—Ç—å, –æ–ø–ª–∞—Ç–∞, —ç–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä...\n",
       "                               ...                        \n",
       "20648    [—Ñ–∏–≥–∞, –ø–∏—Å–∞—Ç—å, —Ä–∞–∑—Ä–∞–±, —á–∏—Ç–∞—Ç—å, –Ω–µ_—É–º–µ—Ç—å, –Ω–µ–≤–æ—Å...\n",
       "20649    [–ø–∏—Å–∞—Ç—å, —Ä—É—Ç–∞, –¥–æ—Å—Ç—É–ø, –æ–±–Ω–æ–≤–∞, —Ä–∞–±–æ—Ç–∞—Ç—å, –æ—Ç–ª–∏—á...\n",
       "20651    [–Ω–æ–≤—ã–π, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å, –æ–ø—Ü–∏—è, —É–ø—Ä–∞...\n",
       "20652    [meizu, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ, –ø–∏—Å–∞—Ç—å, —Ç–µ–ª–æ, —Ä—É—Ç–æ–≤–∞–Ω–Ω—ã–π, ...\n",
       "20654    [—à–ª—è–ø–∞, —Ä–æ–æ—Ç, –ø—Ä–∞–≤–æ, –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–π, –ø—Ä–æ–≥–∞, —Ä–∞–∑—Ä–∞...\n",
       "Name: lda_tokens, Length: 5260, dtype: object"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.vec_tokens.str.contains('–Ω–µ'), 'lda_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(data.lda_tokens)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.4, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in data.lda_tokens.tolist()]\n",
    "tfidf = gensim.models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=4, id2word=dictionary, passes=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.160*\"—É–¥–æ–±–Ω–æ\" + 0.033*\"–±—ã—Å—Ç—Ä–æ\" + 0.032*\"—Ä–∞–±–æ—Ç–∞—Ç—å\" + 0.015*\"–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è\" + 0.015*\"–º–æ–ª–æ–¥–µ—Ü\" + 0.015*\"–∫—Ä—É—Ç–æ\" + 0.012*\"–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\" + 0.010*\"–Ω–µ_–º–æ—á—å\" + 0.008*\"–ø—Ä–æ–±–ª–µ–º–∞\" + 0.008*\"–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ\"\n",
      "Topic: 1 \n",
      "Words: 0.093*\"–Ω—Ä–∞–≤–∏—Ç—å—Å—è\" + 0.092*\"—Å—É–ø–µ—Ä\" + 0.079*\"–æ—Ç–ª–∏—á–Ω—ã–π\" + 0.056*\"—É—Å—Ç—Ä–∞–∏–≤–∞—Ç—å\" + 0.045*\"–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\" + 0.025*\"—É–¥–æ–±–Ω–æ\" + 0.023*\"–¥–æ–≤–æ–ª—å–Ω—ã–π\" + 0.022*\"–ø–æ–Ω—è—Ç–Ω–æ\" + 0.013*\"–ø–æ–Ω—Ä–∞–≤–∏—Ç—å—Å—è\" + 0.012*\"—Ä–∞–±–æ—Ç–∞—Ç—å\"\n",
      "Topic: 2 \n",
      "Words: 0.085*\"–æ—Ç–ª–∏—á–Ω–æ\" + 0.018*\"–∫–ª–∞—Å—Å\" + 0.018*\"—Ä–∞–±–æ—Ç–∞—Ç—å\" + 0.012*\"–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\" + 0.012*\"–ø–∞—Ä–æ–ª—å\" + 0.009*\"–ø–µ—Ä–µ–≤–æ–¥\" + 0.008*\"–≤–≤–æ–¥–∏—Ç—å\" + 0.008*\"–∫–∞—Ä—Ç–∞\" + 0.008*\"–≤—Ö–æ–¥\" + 0.008*\"–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ\"\n",
      "Topic: 3 \n",
      "Words: 0.077*\"—É–¥–æ–±–Ω—ã–π\" + 0.068*\"–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\" + 0.059*\"—Ö–æ—Ä–æ—à–∏–π\" + 0.033*\"–Ω–æ—Ä–º–∞\" + 0.018*\"–Ω–æ—Ä–º–∞–ª—å–Ω–æ\" + 0.010*\"–±–æ–ª—å—à–æ–π\" + 0.009*\"–∞–Ω—Ç–∏–≤–∏—Ä—É—Å\" + 0.009*\"–±–∞–Ω–∫\" + 0.008*\"–ø—Ä–æ—à–∏–≤–∫–∞\" + 0.008*\"–ø—Ä–æ–≥—Ä–∞–º–º–∞\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
