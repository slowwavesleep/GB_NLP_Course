{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../data/tweet_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "- Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "- Исключим стоп-слова с помощью stop_words='english'.\n",
    "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_stemmed_str'] = data.tweet_stemmed.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_lemmatized_str'] = data.tweet_lemmatized.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_stem = CountVectorizer(max_df=0.9,\n",
    "                     max_features=1000,\n",
    "                     stop_words='english',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_feats = cv_stem.fit_transform(data.tweet_stemmed_str.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abl',\n",
       " 'absolut',\n",
       " 'accept',\n",
       " 'account',\n",
       " 'act',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'ad',\n",
       " 'adapt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_stem.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lem = CountVectorizer(max_df=0.9,\n",
    "                    max_features=1000,\n",
    "                    stop_words='english'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_feats = cv_lem.fit_transform(data.tweet_lemmatized_str.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'absolutely',\n",
       " 'account',\n",
       " 'act',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actually',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'adventure']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_lem.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "- Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "- Исключим стоп-слова с помощью stop_words='english'.\n",
    "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_stem = TfidfVectorizer(max_df=0.9,\n",
    "                     max_features=1000,\n",
    "                     stop_words='english',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_feats_tf = tf_stem.fit_transform(data.tweet_stemmed_str.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abl',\n",
       " 'absolut',\n",
       " 'accept',\n",
       " 'account',\n",
       " 'act',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'ad',\n",
       " 'adapt']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_stem.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_lem = TfidfVectorizer(max_df=0.9,\n",
    "                    max_features=1000,\n",
    "                    stop_words='english'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_feats_tf = tf_lem.fit_transform(data.tweet_lemmatized_str.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'absolutely',\n",
       " 'account',\n",
       " 'act',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actually',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'adventure']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_lem.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Натренируем gensim.models.Word2Vec модель на наших данных.\n",
    "- Тренировать будем на токенизированных твитах combine_df['tweet_token']\n",
    "- Установим следующие параметры: size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34.\n",
    "- Используем функцию train() с параметром total_examples равным длине combine_df['tweet_token'], количество epochs установим 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(data.tweet_token.values,\n",
    "               size=200,\n",
    "               window=5,\n",
    "               min_count=2,\n",
    "               sg=1,\n",
    "               hs=0,\n",
    "               negative=10,\n",
    "               workers=32,\n",
    "               seed=32\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9142895, 11726480)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.train(data.tweet_token.values,\n",
    "          total_examples=len(data),\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово positive = \"dinner\", и модель вытащит из корпуса наиболее похожие слова c помощью функции most_similar. То же самое попробуем со словом \"trump\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bolognese', 0.5524142980575562),\n",
       " ('tacotuesday', 0.5496783256530762),\n",
       " ('spaghetti', 0.5425055027008057),\n",
       " ('bihdaydinner', 0.5421866774559021),\n",
       " ('lastnight', 0.5399400591850281),\n",
       " ('cookout', 0.5390915870666504),\n",
       " ('waterloo', 0.5323007106781006),\n",
       " ('burritos', 0.5296170115470886),\n",
       " ('shawarma', 0.5272613763809204),\n",
       " ('hamburger', 0.5270951390266418)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive='dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 0.5491557121276855),\n",
       " ('suppoer', 0.5398991703987122),\n",
       " ('dumptrump', 0.5372797250747681),\n",
       " ('unfavorability', 0.5350761413574219),\n",
       " ('unfit', 0.5135772228240967),\n",
       " ('fuhered', 0.5135111212730408),\n",
       " ('impeachment', 0.5092504620552063),\n",
       " ('conman', 0.5080709457397461),\n",
       " ('delegaterevolt', 0.5072208046913147),\n",
       " ('trumptrain', 0.5045779943466187)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive='trump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова).\n",
    "Давайте проверим векторное представление любого слова из нашего корпуса, например \"food\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05475893, -0.11356499, -0.33500928, -0.22201855, -0.08184546,\n",
       "       -0.35899347,  0.13268024,  0.7418163 , -0.72486985, -1.056604  ,\n",
       "        0.42628524,  0.21999115,  0.0818869 , -0.2868667 ,  0.55796653,\n",
       "        0.04066201, -0.11117391, -0.0210152 ,  0.51768684,  0.12553422,\n",
       "       -0.46531788, -0.1138528 ,  0.08778603,  0.2758265 ,  0.7111261 ,\n",
       "        0.42839685, -0.5635163 , -0.36028975, -0.20995937,  0.125488  ,\n",
       "       -0.3707907 , -0.05812767,  0.07899031, -0.6262374 , -0.22745773,\n",
       "       -0.67574877, -0.39602834,  0.40644497,  0.47129065, -0.33139476,\n",
       "        0.1942984 , -0.15358864, -0.49003208,  0.24558648, -0.0761793 ,\n",
       "       -0.26259258, -0.149178  ,  0.11961325,  0.03037523,  0.26894894,\n",
       "       -0.23119968,  0.14255026,  0.21730103, -0.42587596,  0.76035833,\n",
       "       -0.24758583,  0.38623425, -0.23524559, -0.24236014,  0.818307  ,\n",
       "        0.2021027 ,  0.7091097 ,  0.31673628, -0.01985626, -0.12568079,\n",
       "        0.0824469 ,  0.08431333, -0.31600615, -0.5088694 , -0.3421618 ,\n",
       "        0.13810055, -0.68275326,  0.3590855 ,  0.53909653, -0.39517882,\n",
       "       -0.7981789 , -0.82820624,  0.92019296,  0.7739802 , -0.5875223 ,\n",
       "       -0.2780337 ,  0.5356249 , -0.67787874,  0.30624   ,  0.04440105,\n",
       "       -0.6924796 , -0.7868078 , -0.60401046,  0.19540162, -0.06715913,\n",
       "       -0.3334292 ,  0.04079902,  0.12401721,  0.530219  , -0.29390994,\n",
       "       -0.39389616, -0.13873933, -0.62475234, -0.5558628 ,  0.09727516,\n",
       "       -0.00328091, -0.5510062 , -0.16170336,  0.13925104,  0.2416406 ,\n",
       "       -0.85197526, -0.57938457,  0.09558914, -0.23353247,  0.45494503,\n",
       "        0.05778962, -0.07755992,  0.24635188, -0.06595486, -0.65765893,\n",
       "       -0.6341147 , -0.10046783, -0.89239854,  0.45271382,  0.1318381 ,\n",
       "        0.07859847,  0.2151876 , -0.08024482, -0.30496308,  0.11272632,\n",
       "       -0.03344261,  0.50348777, -0.31195512, -0.06510383,  0.7169184 ,\n",
       "       -0.07336943, -0.82432157,  0.566231  , -0.0326728 ,  0.01378779,\n",
       "       -0.10388722,  0.8373047 ,  0.3138339 ,  0.10317444,  0.13220768,\n",
       "       -0.16422871,  0.21617647,  0.11452811, -0.20903161, -0.13002832,\n",
       "       -0.6582889 ,  0.14468767, -0.09997849, -0.62863934, -0.15417813,\n",
       "        0.23059411,  0.8248862 ,  0.02201524, -0.23179448, -0.06666882,\n",
       "       -0.44754952, -0.20517136,  0.03513865,  0.2196485 , -0.06255928,\n",
       "       -0.60429806,  0.05092379, -0.62051225, -0.8178124 ,  0.3610198 ,\n",
       "        0.01829858, -0.22566992, -0.26280257,  0.2826519 , -0.26351783,\n",
       "        0.4643882 ,  0.47622618,  0.11935046, -0.3610527 , -0.9641852 ,\n",
       "       -0.25557333,  0.3776582 ,  0.84657776,  0.57842493,  0.65531456,\n",
       "        0.09282896,  0.5491629 , -0.12461214, -0.16712143,  0.3499619 ,\n",
       "       -0.47843152,  0.9905094 , -0.11212727, -0.5846699 ,  0.3008074 ,\n",
       "        0.00717256, -0.5326005 , -0.02091979,  0.4431395 , -0.47051337,\n",
       "        0.48138177, -0.49449295, -0.08054001, -0.10994139,  0.6218311 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv['food']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите. Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. Теперь у нас есть 200 функций word2vec для наших данных.\n",
    "Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите. В цикле сделать:  `vec += model_w2v[word].reshape((1, size))`\n",
    "и поделить финальный вектор на количество слов в твите.\n",
    "На выходе должен получиться `wordvec_df.shape = (49159, 200)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_vec(tweet_tokens):\n",
    "    vec = np.zeros(200)\n",
    "    for token in tweet_tokens:\n",
    "        if token in w2v.wv.vocab.keys():\n",
    "            vec += w2v.wv[token]\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['w2v'] = data.tweet_token.apply(lambda x: get_tweet_vec(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объект желаемой формы можно получить следующим образом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_w2v = np.vstack(data.w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 200)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Примечание*: Логика опять нарушена. Нет смысла еще раз фильтровать стоп слова, т.к. они уже были отфильтрованы в предыдущем задании. Кроме того, нигде не говорилось, что результат стемминга и лематизации нужно сохранять в виде строк (а не списков токенов), которых требуют `TfIdfVectorizer` и `CountVectorizer` с параметрами в задании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
