{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"HW7_attention.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0U8FyfUoFkGB","colab_type":"text"},"source":["Запустить seq2seq, seq2seq с вниманием и трансформер для перевода русских слов и описать наблюдения по качеству.\n"]},{"cell_type":"code","metadata":{"id":"QfBh4itWHyH1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597137783760,"user_tz":-180,"elapsed":942,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["%tensorflow_version 2.x"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"dzh62pIVFkGD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597138132130,"user_tz":-180,"elapsed":946,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense\n","import numpy as np\n","import re"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"QOe3SPnBFkGJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597137792036,"user_tz":-180,"elapsed":878,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["batch_size = 128\n","epochs = 100\n","latent_dim = 256\n","num_samples = 10000\n","data_path = 'data/rus-eng/rus.txt'"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"bM9pkp61FkGO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597137792581,"user_tz":-180,"elapsed":628,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text, _ = line.split('\\t')\n","    target_text = '\\t' + target_text + '\\n'\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"AW0QKYOjFkGS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597137793747,"user_tz":-180,"elapsed":785,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["input_token_index = dict(\n","    [(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict(\n","    [(char, i) for i, char in enumerate(target_characters)])"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3oLqovbFkGX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597137795421,"user_tz":-180,"elapsed":975,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float32')\n","decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Alv6SHEPFkGc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597137797105,"user_tz":-180,"elapsed":1265,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.\n","    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n","    for t, char in enumerate(target_text):\n","        decoder_input_data[i, t, target_token_index[char]] = 1.\n","        if t > 0:\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n","    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n","    decoder_target_data[i, t:, target_token_index[' ']] = 1."],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"uYZfm-a8FkGg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597138131150,"user_tz":-180,"elapsed":334187,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"171d3dd9-e1fd-4020-b7a2-fa09cfd412e7"},"source":["encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","encoder_states = [state_h, state_c]\n","\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_split=0.2)\n","\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)\n","\n","reverse_input_char_index = dict(\n","    (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","    (i, char) for char, i in target_token_index.items())\n","\n","\n","def decode_sequence(input_seq):\n","    states_value = encoder_model.predict(input_seq)\n","\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        states_value = [h, c]\n","\n","    return decoded_sentence\n","\n","\n","for seq_index in range(100):\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","63/63 [==============================] - 3s 55ms/step - loss: 1.3980 - accuracy: 0.7633 - val_loss: 1.0769 - val_accuracy: 0.7555\n","Epoch 2/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.8661 - accuracy: 0.7810 - val_loss: 0.9058 - val_accuracy: 0.7599\n","Epoch 3/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.7629 - accuracy: 0.7916 - val_loss: 0.8297 - val_accuracy: 0.7717\n","Epoch 4/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.7013 - accuracy: 0.8149 - val_loss: 0.7759 - val_accuracy: 0.7940\n","Epoch 5/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.6499 - accuracy: 0.8296 - val_loss: 0.7189 - val_accuracy: 0.8081\n","Epoch 6/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.6037 - accuracy: 0.8390 - val_loss: 0.6800 - val_accuracy: 0.8145\n","Epoch 7/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.5677 - accuracy: 0.8450 - val_loss: 0.6430 - val_accuracy: 0.8241\n","Epoch 8/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.6093 - accuracy: 0.8414 - val_loss: 0.9415 - val_accuracy: 0.7683\n","Epoch 9/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.6092 - accuracy: 0.8357 - val_loss: 0.6539 - val_accuracy: 0.8196\n","Epoch 10/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.5486 - accuracy: 0.8475 - val_loss: 0.6254 - val_accuracy: 0.8249\n","Epoch 11/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.5292 - accuracy: 0.8506 - val_loss: 0.6132 - val_accuracy: 0.8260\n","Epoch 12/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.5161 - accuracy: 0.8536 - val_loss: 0.5977 - val_accuracy: 0.8288\n","Epoch 13/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.5058 - accuracy: 0.8558 - val_loss: 0.5862 - val_accuracy: 0.8317\n","Epoch 14/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4961 - accuracy: 0.8582 - val_loss: 0.5772 - val_accuracy: 0.8354\n","Epoch 15/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4883 - accuracy: 0.8599 - val_loss: 0.5689 - val_accuracy: 0.8366\n","Epoch 16/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4805 - accuracy: 0.8618 - val_loss: 0.5592 - val_accuracy: 0.8394\n","Epoch 17/100\n","63/63 [==============================] - 3s 40ms/step - loss: 0.4728 - accuracy: 0.8637 - val_loss: 0.5527 - val_accuracy: 0.8404\n","Epoch 18/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4659 - accuracy: 0.8655 - val_loss: 0.5449 - val_accuracy: 0.8416\n","Epoch 19/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4584 - accuracy: 0.8674 - val_loss: 0.5431 - val_accuracy: 0.8424\n","Epoch 20/100\n","63/63 [==============================] - 3s 40ms/step - loss: 0.4515 - accuracy: 0.8695 - val_loss: 0.5316 - val_accuracy: 0.8462\n","Epoch 21/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4447 - accuracy: 0.8710 - val_loss: 0.5281 - val_accuracy: 0.8471\n","Epoch 22/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4377 - accuracy: 0.8730 - val_loss: 0.5242 - val_accuracy: 0.8479\n","Epoch 23/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4327 - accuracy: 0.8744 - val_loss: 0.5161 - val_accuracy: 0.8509\n","Epoch 24/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4260 - accuracy: 0.8762 - val_loss: 0.5099 - val_accuracy: 0.8529\n","Epoch 25/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4200 - accuracy: 0.8780 - val_loss: 0.5054 - val_accuracy: 0.8540\n","Epoch 26/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4831 - accuracy: 0.8632 - val_loss: 0.5417 - val_accuracy: 0.8424\n","Epoch 27/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4433 - accuracy: 0.8716 - val_loss: 0.5215 - val_accuracy: 0.8483\n","Epoch 28/100\n","63/63 [==============================] - 3s 40ms/step - loss: 0.4278 - accuracy: 0.8753 - val_loss: 0.5102 - val_accuracy: 0.8521\n","Epoch 29/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4188 - accuracy: 0.8777 - val_loss: 0.5053 - val_accuracy: 0.8528\n","Epoch 30/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4119 - accuracy: 0.8797 - val_loss: 0.4982 - val_accuracy: 0.8562\n","Epoch 31/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4058 - accuracy: 0.8818 - val_loss: 0.4934 - val_accuracy: 0.8577\n","Epoch 32/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.4004 - accuracy: 0.8833 - val_loss: 0.4906 - val_accuracy: 0.8587\n","Epoch 33/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3944 - accuracy: 0.8851 - val_loss: 0.4877 - val_accuracy: 0.8596\n","Epoch 34/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3888 - accuracy: 0.8868 - val_loss: 0.4835 - val_accuracy: 0.8612\n","Epoch 35/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3844 - accuracy: 0.8878 - val_loss: 0.4795 - val_accuracy: 0.8615\n","Epoch 36/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3793 - accuracy: 0.8891 - val_loss: 0.4759 - val_accuracy: 0.8632\n","Epoch 37/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3744 - accuracy: 0.8905 - val_loss: 0.4728 - val_accuracy: 0.8640\n","Epoch 38/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3696 - accuracy: 0.8921 - val_loss: 0.4716 - val_accuracy: 0.8646\n","Epoch 39/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3645 - accuracy: 0.8935 - val_loss: 0.4672 - val_accuracy: 0.8653\n","Epoch 40/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3602 - accuracy: 0.8949 - val_loss: 0.4673 - val_accuracy: 0.8666\n","Epoch 41/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3560 - accuracy: 0.8962 - val_loss: 0.4645 - val_accuracy: 0.8674\n","Epoch 42/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3513 - accuracy: 0.8975 - val_loss: 0.4599 - val_accuracy: 0.8686\n","Epoch 43/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3459 - accuracy: 0.8991 - val_loss: 0.4591 - val_accuracy: 0.8694\n","Epoch 44/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3414 - accuracy: 0.9004 - val_loss: 0.4570 - val_accuracy: 0.8694\n","Epoch 45/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3371 - accuracy: 0.9017 - val_loss: 0.4541 - val_accuracy: 0.8708\n","Epoch 46/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3328 - accuracy: 0.9030 - val_loss: 0.4546 - val_accuracy: 0.8704\n","Epoch 47/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3276 - accuracy: 0.9047 - val_loss: 0.4534 - val_accuracy: 0.8720\n","Epoch 48/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3234 - accuracy: 0.9055 - val_loss: 0.4509 - val_accuracy: 0.8715\n","Epoch 49/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3200 - accuracy: 0.9066 - val_loss: 0.4518 - val_accuracy: 0.8714\n","Epoch 50/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3154 - accuracy: 0.9080 - val_loss: 0.4477 - val_accuracy: 0.8726\n","Epoch 51/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3101 - accuracy: 0.9095 - val_loss: 0.4461 - val_accuracy: 0.8744\n","Epoch 52/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3056 - accuracy: 0.9110 - val_loss: 0.4449 - val_accuracy: 0.8733\n","Epoch 53/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.3020 - accuracy: 0.9117 - val_loss: 0.4468 - val_accuracy: 0.8746\n","Epoch 54/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2975 - accuracy: 0.9129 - val_loss: 0.4444 - val_accuracy: 0.8747\n","Epoch 55/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2923 - accuracy: 0.9145 - val_loss: 0.4419 - val_accuracy: 0.8754\n","Epoch 56/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2883 - accuracy: 0.9155 - val_loss: 0.4412 - val_accuracy: 0.8759\n","Epoch 57/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2836 - accuracy: 0.9169 - val_loss: 0.4433 - val_accuracy: 0.8746\n","Epoch 58/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2803 - accuracy: 0.9175 - val_loss: 0.4403 - val_accuracy: 0.8768\n","Epoch 59/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2754 - accuracy: 0.9192 - val_loss: 0.4448 - val_accuracy: 0.8753\n","Epoch 60/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2714 - accuracy: 0.9204 - val_loss: 0.4431 - val_accuracy: 0.8766\n","Epoch 61/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2667 - accuracy: 0.9214 - val_loss: 0.4415 - val_accuracy: 0.8764\n","Epoch 62/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2626 - accuracy: 0.9226 - val_loss: 0.4405 - val_accuracy: 0.8771\n","Epoch 63/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2580 - accuracy: 0.9242 - val_loss: 0.4420 - val_accuracy: 0.8775\n","Epoch 64/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2539 - accuracy: 0.9254 - val_loss: 0.4420 - val_accuracy: 0.8772\n","Epoch 65/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2490 - accuracy: 0.9268 - val_loss: 0.4405 - val_accuracy: 0.8782\n","Epoch 66/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2459 - accuracy: 0.9274 - val_loss: 0.4423 - val_accuracy: 0.8782\n","Epoch 67/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2417 - accuracy: 0.9288 - val_loss: 0.4441 - val_accuracy: 0.8771\n","Epoch 68/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2373 - accuracy: 0.9299 - val_loss: 0.4454 - val_accuracy: 0.8782\n","Epoch 69/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2338 - accuracy: 0.9309 - val_loss: 0.4443 - val_accuracy: 0.8787\n","Epoch 70/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2292 - accuracy: 0.9322 - val_loss: 0.4459 - val_accuracy: 0.8784\n","Epoch 71/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2255 - accuracy: 0.9334 - val_loss: 0.4477 - val_accuracy: 0.8785\n","Epoch 72/100\n","63/63 [==============================] - 3s 40ms/step - loss: 0.2204 - accuracy: 0.9351 - val_loss: 0.4496 - val_accuracy: 0.8778\n","Epoch 73/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2174 - accuracy: 0.9355 - val_loss: 0.4508 - val_accuracy: 0.8781\n","Epoch 74/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2130 - accuracy: 0.9370 - val_loss: 0.4494 - val_accuracy: 0.8794\n","Epoch 75/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2097 - accuracy: 0.9381 - val_loss: 0.4491 - val_accuracy: 0.8792\n","Epoch 76/100\n","63/63 [==============================] - 3s 40ms/step - loss: 0.2052 - accuracy: 0.9393 - val_loss: 0.4550 - val_accuracy: 0.8795\n","Epoch 77/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.2009 - accuracy: 0.9406 - val_loss: 0.4550 - val_accuracy: 0.8794\n","Epoch 78/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1985 - accuracy: 0.9412 - val_loss: 0.4565 - val_accuracy: 0.8801\n","Epoch 79/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1936 - accuracy: 0.9427 - val_loss: 0.4624 - val_accuracy: 0.8782\n","Epoch 80/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1903 - accuracy: 0.9437 - val_loss: 0.4624 - val_accuracy: 0.8788\n","Epoch 81/100\n","63/63 [==============================] - 3s 40ms/step - loss: 0.1864 - accuracy: 0.9448 - val_loss: 0.4636 - val_accuracy: 0.8789\n","Epoch 82/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1840 - accuracy: 0.9453 - val_loss: 0.4641 - val_accuracy: 0.8791\n","Epoch 83/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1800 - accuracy: 0.9467 - val_loss: 0.4662 - val_accuracy: 0.8795\n","Epoch 84/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1760 - accuracy: 0.9477 - val_loss: 0.4731 - val_accuracy: 0.8787\n","Epoch 85/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1721 - accuracy: 0.9488 - val_loss: 0.4725 - val_accuracy: 0.8803\n","Epoch 86/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1690 - accuracy: 0.9495 - val_loss: 0.4734 - val_accuracy: 0.8796\n","Epoch 87/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1666 - accuracy: 0.9504 - val_loss: 0.4768 - val_accuracy: 0.8798\n","Epoch 88/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1627 - accuracy: 0.9516 - val_loss: 0.4815 - val_accuracy: 0.8790\n","Epoch 89/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1589 - accuracy: 0.9528 - val_loss: 0.4833 - val_accuracy: 0.8797\n","Epoch 90/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1553 - accuracy: 0.9538 - val_loss: 0.4865 - val_accuracy: 0.8798\n","Epoch 91/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1521 - accuracy: 0.9548 - val_loss: 0.4904 - val_accuracy: 0.8794\n","Epoch 92/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1493 - accuracy: 0.9555 - val_loss: 0.4915 - val_accuracy: 0.8780\n","Epoch 93/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1461 - accuracy: 0.9565 - val_loss: 0.4953 - val_accuracy: 0.8782\n","Epoch 94/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1433 - accuracy: 0.9572 - val_loss: 0.4983 - val_accuracy: 0.8782\n","Epoch 95/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1402 - accuracy: 0.9582 - val_loss: 0.5010 - val_accuracy: 0.8781\n","Epoch 96/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1375 - accuracy: 0.9589 - val_loss: 0.5075 - val_accuracy: 0.8779\n","Epoch 97/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1563 - accuracy: 0.9530 - val_loss: 0.4979 - val_accuracy: 0.8763\n","Epoch 98/100\n","63/63 [==============================] - 3s 42ms/step - loss: 0.1548 - accuracy: 0.9529 - val_loss: 0.4999 - val_accuracy: 0.8788\n","Epoch 99/100\n","63/63 [==============================] - 3s 42ms/step - loss: 0.1360 - accuracy: 0.9590 - val_loss: 0.5060 - val_accuracy: 0.8795\n","Epoch 100/100\n","63/63 [==============================] - 3s 41ms/step - loss: 0.1301 - accuracy: 0.9610 - val_loss: 0.5107 - val_accuracy: 0.8789\n","-\n","Input sentence: Go.\n","Decoded sentence: Идите.\n","\n","-\n","Input sentence: Go.\n","Decoded sentence: Идите.\n","\n","-\n","Input sentence: Go.\n","Decoded sentence: Идите.\n","\n","-\n","Input sentence: Hi.\n","Decoded sentence: Здравствуйте.\n","\n","-\n","Input sentence: Hi.\n","Decoded sentence: Здравствуйте.\n","\n","-\n","Input sentence: Hi.\n","Decoded sentence: Здравствуйте.\n","\n","-\n","Input sentence: Hi.\n","Decoded sentence: Здравствуйте.\n","\n","-\n","Input sentence: Hi.\n","Decoded sentence: Здравствуйте.\n","\n","-\n","Input sentence: Run!\n","Decoded sentence: Бегите!\n","\n","-\n","Input sentence: Run!\n","Decoded sentence: Бегите!\n","\n","-\n","Input sentence: Run.\n","Decoded sentence: Бегите!\n","\n","-\n","Input sentence: Run.\n","Decoded sentence: Бегите!\n","\n","-\n","Input sentence: Who?\n","Decoded sentence: Кто?\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Вхотро!\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Вхотро!\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Вхотро!\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Вхотро!\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Вхотро!\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Вхотро!\n","\n","-\n","Input sentence: Fire!\n","Decoded sentence: Пожар!\n","\n","-\n","Input sentence: Fire!\n","Decoded sentence: Пожар!\n","\n","-\n","Input sentence: Help!\n","Decoded sentence: Помогите!\n","\n","-\n","Input sentence: Help!\n","Decoded sentence: Помогите!\n","\n","-\n","Input sentence: Help!\n","Decoded sentence: Помогите!\n","\n","-\n","Input sentence: Jump!\n","Decoded sentence: Прыгайте!\n","\n","-\n","Input sentence: Jump!\n","Decoded sentence: Прыгайте!\n","\n","-\n","Input sentence: Jump.\n","Decoded sentence: Прыгайте!\n","\n","-\n","Input sentence: Jump.\n","Decoded sentence: Прыгайте!\n","\n","-\n","Input sentence: Stop!\n","Decoded sentence: Остановись!\n","\n","-\n","Input sentence: Stop!\n","Decoded sentence: Остановись!\n","\n","-\n","Input sentence: Stop!\n","Decoded sentence: Остановись!\n","\n","-\n","Input sentence: Wait!\n","Decoded sentence: Подожди!\n","\n","-\n","Input sentence: Wait!\n","Decoded sentence: Подожди!\n","\n","-\n","Input sentence: Wait!\n","Decoded sentence: Подожди!\n","\n","-\n","Input sentence: Wait!\n","Decoded sentence: Подожди!\n","\n","-\n","Input sentence: Wait.\n","Decoded sentence: Ждите.\n","\n","-\n","Input sentence: Wait.\n","Decoded sentence: Ждите.\n","\n","-\n","Input sentence: Wait.\n","Decoded sentence: Ждите.\n","\n","-\n","Input sentence: Do it.\n","Decoded sentence: Сделай это.\n","\n","-\n","Input sentence: Go on.\n","Decoded sentence: Продолжайте.\n","\n","-\n","Input sentence: Go on.\n","Decoded sentence: Продолжайте.\n","\n","-\n","Input sentence: Hello!\n","Decoded sentence: Привет!\n","\n","-\n","Input sentence: Hello!\n","Decoded sentence: Привет!\n","\n","-\n","Input sentence: Hello!\n","Decoded sentence: Привет!\n","\n","-\n","Input sentence: Hello!\n","Decoded sentence: Привет!\n","\n","-\n","Input sentence: Hurry!\n","Decoded sentence: Пократи!\n","\n","-\n","Input sentence: I ran.\n","Decoded sentence: Я побежала.\n","\n","-\n","Input sentence: I ran.\n","Decoded sentence: Я побежала.\n","\n","-\n","Input sentence: I ran.\n","Decoded sentence: Я побежала.\n","\n","-\n","Input sentence: I ran.\n","Decoded sentence: Я побежала.\n","\n","-\n","Input sentence: I see.\n","Decoded sentence: Я провал.\n","\n","-\n","Input sentence: I see.\n","Decoded sentence: Я провал.\n","\n","-\n","Input sentence: I see.\n","Decoded sentence: Я провал.\n","\n","-\n","Input sentence: I try.\n","Decoded sentence: Я пытаюсь.\n","\n","-\n","Input sentence: I try.\n","Decoded sentence: Я пытаюсь.\n","\n","-\n","Input sentence: I try.\n","Decoded sentence: Я пытаюсь.\n","\n","-\n","Input sentence: I won!\n","Decoded sentence: Я победила!\n","\n","-\n","Input sentence: I won!\n","Decoded sentence: Я победила!\n","\n","-\n","Input sentence: I won!\n","Decoded sentence: Я победила!\n","\n","-\n","Input sentence: I won!\n","Decoded sentence: Я победила!\n","\n","-\n","Input sentence: Oh no!\n","Decoded sentence: О нет ны ваё.\n","\n","-\n","Input sentence: Relax.\n","Decoded sentence: Расслабьтесь.\n","\n","-\n","Input sentence: Relax.\n","Decoded sentence: Расслабьтесь.\n","\n","-\n","Input sentence: Relax.\n","Decoded sentence: Расслабьтесь.\n","\n","-\n","Input sentence: Shoot!\n","Decoded sentence: Сдал.\n","\n","-\n","Input sentence: Smile.\n","Decoded sentence: Улыбнись.\n","\n","-\n","Input sentence: Smile.\n","Decoded sentence: Улыбнись.\n","\n","-\n","Input sentence: Smile.\n","Decoded sentence: Улыбнись.\n","\n","-\n","Input sentence: Smile.\n","Decoded sentence: Улыбнись.\n","\n","-\n","Input sentence: Smile.\n","Decoded sentence: Улыбнись.\n","\n","-\n","Input sentence: Smile.\n","Decoded sentence: Улыбнись.\n","\n","-\n","Input sentence: Attack!\n","Decoded sentence: Подажай за больно.\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: Ваше здоровье!\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: Ваше здоровье!\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: Ваше здоровье!\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: Ваше здоровье!\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: Ваше здоровье!\n","\n","-\n","Input sentence: Eat it.\n","Decoded sentence: Съешь это.\n","\n","-\n","Input sentence: Eat up.\n","Decoded sentence: Подожай на мно.\n","\n","-\n","Input sentence: Freeze!\n","Decoded sentence: Ни спеша.\n","\n","-\n","Input sentence: Freeze!\n","Decoded sentence: Ни спеша.\n","\n","-\n","Input sentence: Freeze!\n","Decoded sentence: Ни спеша.\n","\n","-\n","Input sentence: Freeze!\n","Decoded sentence: Ни спеша.\n","\n","-\n","Input sentence: Get up.\n","Decoded sentence: Поднимайся.\n","\n","-\n","Input sentence: Get up.\n","Decoded sentence: Поднимайся.\n","\n","-\n","Input sentence: Get up.\n","Decoded sentence: Поднимайся.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Иди уете.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Иди уете.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Иди уете.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Иди уете.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Иди уете.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Иди уете.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Иди уете.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Иди уете.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Иди уете.\n","\n","-\n","Input sentence: Got it!\n","Decoded sentence: Усанак!\n","\n","-\n","Input sentence: Got it?\n","Decoded sentence: Усеклай?\n","\n","-\n","Input sentence: Got it?\n","Decoded sentence: Усеклай?\n","\n","-\n","Input sentence: Got it?\n","Decoded sentence: Усеклай?\n","\n","-\n","Input sentence: Got it?\n","Decoded sentence: Усеклай?\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aPiRo16J1opd","colab_type":"text"},"source":["Результаты получаются странноватые. Каждая последовательность всегда переводится одинаково. Пунктуация сильно меняет результат."]},{"cell_type":"markdown","metadata":{"id":"DuPdVhFPBcnm","colab_type":"text"},"source":["Пришлось поправить токенизацию."]},{"cell_type":"code","metadata":{"id":"i0sHM9FtFkGn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597140960797,"user_tz":-180,"elapsed":3500,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["import pandas as pd\n","\n","data_path = 'data/rus-eng/rus.txt'\n","num_samples = 10000\n","\n","\n","ru_symbols = set()\n","_ = pd.read_csv(data_path, sep='\\t', header=None)[1].apply(lambda x: ru_symbols.update(x))\n","\n","symbs = \"\".join([sym for sym in ru_symbols if sym.isalpha()])\n","\n","\n","# в 2.x включено по дефолту\n","# tf.enable_eager_execution()\n","\n","input_texts = []\n","target_texts = []\n","\n","def preprocess_sentence(w):\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","    w = re.sub(rf\"[^a-zA-Z?.!,¿{symbs}]+\", \" \", w)\n","    w = w.strip()\n","    w = '<start> ' + w + ' <end>'\n","    return w\n","\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text, _ = line.split('\\t')\n","    target_text = '\\t' + target_text + '\\n'\n","    input_texts.append(preprocess_sentence(input_text))\n","    target_texts.append(preprocess_sentence(target_text))"],"execution_count":89,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKyaF-guFkGr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597140960799,"user_tz":-180,"elapsed":3017,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["def tokenize(lang):\n","    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      filters='')\n","    lang_tokenizer.fit_on_texts(lang)\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","    return tensor, lang_tokenizer"],"execution_count":90,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQAxSGx3FkGu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597140961240,"user_tz":-180,"elapsed":2888,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n","target_tensor, targ_lang_tokenizer = tokenize(target_texts)"],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"id":"dgWJOprRFkGy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597140961241,"user_tz":-180,"elapsed":2417,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"],"execution_count":92,"outputs":[]},{"cell_type":"code","metadata":{"id":"QiWJOhvLFkG2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597140961242,"user_tz":-180,"elapsed":1947,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 128\n","steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","\n","vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n","vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":93,"outputs":[]},{"cell_type":"code","metadata":{"id":"PM6Ke5ODFkG6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597140961244,"user_tz":-180,"elapsed":1445,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","        super(Encoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = tf.keras.layers.GRU(self.enc_units,\n","                                       return_sequences=True,\n","                                       return_state=True)\n","\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state = self.lstm(x, initial_state = hidden)\n","        return output, state\n","\n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_sz, self.enc_units))\n","\n","    \n","class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, query, values):\n","        query_with_time_axis = tf.expand_dims(query, 1)\n","        score = self.V(tf.nn.tanh(\n","            self.W1(query_with_time_axis) + self.W2(values)))\n","\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        context_vector = attention_weights * values\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","        return context_vector, attention_weights\n","    \n","    \n","class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","        super(Decoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = tf.keras.layers.GRU(self.dec_units,\n","                                       return_sequences=True,\n","                                       return_state=True)\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","        self.attention = BahdanauAttention(self.dec_units)\n","\n","    def call(self, x, hidden, enc_output):\n","        context_vector, attention_weights = self.attention(hidden, enc_output)\n","        x = self.embedding(x)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        output, state = self.lstm(x)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        x = self.fc(output)\n","        return x, state, attention_weights"],"execution_count":94,"outputs":[]},{"cell_type":"code","metadata":{"id":"rR-u3_UyFkG9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597140961245,"user_tz":-180,"elapsed":1037,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","\n","optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)"],"execution_count":95,"outputs":[]},{"cell_type":"code","metadata":{"id":"kN-lCtDkFkHA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597140961612,"user_tz":-180,"elapsed":924,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["@tf.function\n","def train_step(inp, targ, enc_hidden):\n","    loss = 0\n","\n","    with tf.GradientTape() as tape:\n","        enc_output, enc_hidden = encoder(inp, enc_hidden)\n","        dec_hidden = enc_hidden\n","        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","        for t in range(1, targ.shape[1]):\n","            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","            loss += loss_function(targ[:, t], predictions)\n","            dec_input = tf.expand_dims(targ[:, t], 1)\n","    \n","    batch_loss = (loss / int(targ.shape[1]))\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","    return batch_loss"],"execution_count":96,"outputs":[]},{"cell_type":"code","metadata":{"id":"QIVfLFLnFkHE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"ok","timestamp":1597141211327,"user_tz":-180,"elapsed":174463,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"1c40200e-9617-4966-b39c-241d65c804ab"},"source":["EPOCHS = 15\n","for epoch in range(EPOCHS):\n","    enc_hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","\n","    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","        batch_loss = train_step(inp, targ, enc_hidden)\n","        total_loss += batch_loss\n","    \n","    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss / steps_per_epoch))"],"execution_count":98,"outputs":[{"output_type":"stream","text":["Epoch 1 Loss 0.8551\n","Epoch 2 Loss 0.7715\n","Epoch 3 Loss 0.6858\n","Epoch 4 Loss 0.6072\n","Epoch 5 Loss 0.5376\n","Epoch 6 Loss 0.4710\n","Epoch 7 Loss 0.4124\n","Epoch 8 Loss 0.3616\n","Epoch 9 Loss 0.3159\n","Epoch 10 Loss 0.2784\n","Epoch 11 Loss 0.2473\n","Epoch 12 Loss 0.2239\n","Epoch 13 Loss 0.2061\n","Epoch 14 Loss 0.1915\n","Epoch 15 Loss 0.1779\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l7sLqa4aFkHI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141244920,"user_tz":-180,"elapsed":889,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n","\n","def evaluate(sentence):\n","    attention_plot = np.zeros((max_length_targ, max_length_inp))\n","    sentence = preprocess_sentence(sentence)\n","    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                         maxlen=max_length_inp,\n","                                                         padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    result = ''\n","    hidden = [tf.zeros((1, units))]\n","    enc_out, enc_hidden = encoder(inputs, hidden)\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n","\n","    for t in range(max_length_targ):\n","        predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                             dec_hidden,\n","                                                             enc_out)\n","        attention_weights = tf.reshape(attention_weights, (-1, ))\n","        attention_plot[t] = attention_weights.numpy()\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n","\n","        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n","            return result, sentence, attention_plot\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","    return result, sentence, attention_plot\n","\n","def plot_attention(attention, sentence, predicted_sentence):\n","    fig = plt.figure(figsize=(10,10))\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.matshow(attention, cmap='viridis')\n","    fontdict = {'fontsize': 14}\n","    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n","    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n","    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","    plt.show()\n","    \n","def translate(sentence):\n","    result, sentence, attention_plot = evaluate(sentence)\n","    print('Input: %s' % (sentence))\n","    print('Predicted translation: {}'.format(result))\n","    # attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n","    # plot_attention(attention_plot, sentence.split(' '), result.split(' '))"],"execution_count":99,"outputs":[]},{"cell_type":"code","metadata":{"id":"fqN8RtI29KsT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597141291378,"user_tz":-180,"elapsed":1004,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"54dbe4e4-3452-4590-e38a-95c4edb6a222"},"source":["translate('good morning')"],"execution_count":103,"outputs":[{"output_type":"stream","text":["Input: <start> good morning <end>\n","Predicted translation: до встречи . <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fAndJyx2CiWC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597141309977,"user_tz":-180,"elapsed":812,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"69a4eb39-09a0-45fc-b2ce-978f81f84863"},"source":["translate(\"hello\")"],"execution_count":104,"outputs":[{"output_type":"stream","text":["Input: <start> hello <end>\n","Predicted translation: здравствуйте . <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-UeodEifCyhe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597141368573,"user_tz":-180,"elapsed":797,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"a9fa5e01-82be-4c96-f464-c29a49fecf20"},"source":["translate(\"go?\")"],"execution_count":107,"outputs":[{"output_type":"stream","text":["Input: <start> go ? <end>\n","Predicted translation: есть ? <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jie85mNQC3hB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597141395257,"user_tz":-180,"elapsed":809,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"ea6f1923-e4bf-45ec-e17d-f319553e5aa1"},"source":["translate(\"got it?\")"],"execution_count":108,"outputs":[{"output_type":"stream","text":["Input: <start> got it ? <end>\n","Predicted translation: ферштейн ? <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4737ET6bC9JH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597141417032,"user_tz":-180,"elapsed":977,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"d8a03c4e-7286-4069-88c6-dd8660b735ad"},"source":["translate(\"got it.\")"],"execution_count":109,"outputs":[{"output_type":"stream","text":["Input: <start> got it . <end>\n","Predicted translation: проверьте это . <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7fY0hS0CCnwT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597141349700,"user_tz":-180,"elapsed":803,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"40bcd343-93d8-4d7f-fb0d-80aa8659a3f8"},"source":["translate(\"go.\")"],"execution_count":106,"outputs":[{"output_type":"stream","text":["Input: <start> go . <end>\n","Predicted translation: иди . <end> \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mxLFzOd8DIjC","colab_type":"text"},"source":["Здесь, как минимум, не появляется всяких странных слов."]},{"cell_type":"code","metadata":{"id":"6C_VpWuiFkHO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141524028,"user_tz":-180,"elapsed":1134,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","    \"\"\"Calculate the attention weights.\n","  q, k, v must have matching leading dimensions.\n","  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","  The mask has different shapes depending on its type(padding or look ahead) \n","  but it must be broadcastable for addition.\n","  \n","  Args:\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable \n","          to (..., seq_len_q, seq_len_k). Defaults to None.\n","    \n","  Returns:\n","    output, attention_weights\n","    \"\"\"\n","\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","  \n","  # scale matmul_qk\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # add the mask to the scaled tensor.\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)  \n","\n","  # softmax is normalized on the last axis (seq_len_k) so that the scores\n","  # add up to 1.\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","    return output, attention_weights"],"execution_count":110,"outputs":[]},{"cell_type":"code","metadata":{"id":"K_c9d32kFkHR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141524029,"user_tz":-180,"elapsed":616,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","        \n","    def split_heads(self, x, batch_size):\n","        \"\"\"Split the last dimension into (num_heads, depth).\n","        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","        \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)  # (batch_size, seq_len, d_model)\n","        k = self.wk(k)  # (batch_size, seq_len, d_model)\n","        v = self.wv(v)  # (batch_size, seq_len, d_model)\n","\n","        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention, \n","                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","        return output, attention_weights\n","    \n","def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","  ])"],"execution_count":111,"outputs":[]},{"cell_type":"code","metadata":{"id":"opjV8EmeFkHW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141525198,"user_tz":-180,"elapsed":1340,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, training, mask):\n","\n","        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        return out2\n","    \n","class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","    \n","    def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","        # enc_output.shape == (batch_size, input_seq_len, d_model)\n","\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","\n","        attn2, attn_weights_block2 = self.mha2(\n","            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n","\n","        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n","\n","        return out3, attn_weights_block1, attn_weights_block2\n","    \n","    \n","class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, \n","                                                self.d_model)\n","\n","\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n","                           for _ in range(num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","        \n","    def call(self, x, training, mask):\n","\n","        seq_len = tf.shape(x)[1]\n","\n","        # adding embedding and position encoding.\n","        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training, mask)\n","\n","        return x  # (batch_size, input_seq_len, d_model)"],"execution_count":112,"outputs":[]},{"cell_type":"code","metadata":{"id":"99htF9Y0FkHa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141525200,"user_tz":-180,"elapsed":1050,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n","                           for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","\n","        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n","                                                 look_ahead_mask, padding_mask)\n","\n","            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","\n","        # x.shape == (batch_size, target_seq_len, d_model)\n","        return x, attention_weights"],"execution_count":113,"outputs":[]},{"cell_type":"code","metadata":{"id":"eya4uWd_FkHd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141525760,"user_tz":-180,"elapsed":1212,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n","               target_vocab_size, pe_input, pe_target, rate=0.1):\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n","                               input_vocab_size, pe_input, rate)\n","\n","        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n","                               target_vocab_size, pe_target, rate)\n","\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    \n","    def call(self, inp, tar, training, enc_padding_mask, \n","           look_ahead_mask, dec_padding_mask):\n","\n","        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","\n","        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","        dec_output, attention_weights = self.decoder(\n","            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","\n","        return final_output, attention_weights"],"execution_count":114,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1If7WFtFkHg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141526280,"user_tz":-180,"elapsed":1161,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","\n","input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n","target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n","dropout_rate = 0.1"],"execution_count":115,"outputs":[]},{"cell_type":"code","metadata":{"id":"hg6WiZK9FkHj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141526671,"user_tz":-180,"elapsed":990,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","        self.warmup_steps = warmup_steps\n","    \n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":116,"outputs":[]},{"cell_type":"code","metadata":{"id":"8j_Gt5HeFkHp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141527561,"user_tz":-180,"elapsed":1136,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["learning_rate = CustomSchedule(d_model)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n","                                     epsilon=1e-9)\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","  \n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"],"execution_count":117,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdYZl-75FkHt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141528399,"user_tz":-180,"elapsed":1379,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","  \n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","  \n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    \n","    pos_encoding = angle_rads[np.newaxis, ...]\n","    \n","    return tf.cast(pos_encoding, dtype=tf.float32)"],"execution_count":118,"outputs":[]},{"cell_type":"code","metadata":{"id":"deEaUaimFkHw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141530143,"user_tz":-180,"elapsed":1487,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n","    name='train_accuracy')\n","\n","transformer = Transformer(num_layers, d_model, num_heads, dff,\n","                          input_vocab_size, target_vocab_size, \n","                          pe_input=input_vocab_size, \n","                          pe_target=target_vocab_size,\n","                          rate=dropout_rate)"],"execution_count":119,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xuxwqVlFkHz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141530144,"user_tz":-180,"elapsed":821,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["def create_masks(inp, tar):\n","    enc_padding_mask = create_padding_mask(inp)\n","  \n","    dec_padding_mask = create_padding_mask(inp)\n","  \n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","  \n","    return enc_padding_mask, combined_mask, dec_padding_mask"],"execution_count":120,"outputs":[]},{"cell_type":"code","metadata":{"id":"z2wLmXs0FkH2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597141530590,"user_tz":-180,"elapsed":551,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}}},"source":["\n","train_step_signature = [\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n","]\n","\n","@tf.function(input_signature=train_step_signature)\n","def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","  \n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","  \n","    with tf.GradientTape() as tape:\n","        predictions, _ = transformer(inp, tar_inp, \n","                                 True, \n","                                 enc_padding_mask, \n","                                 combined_mask, \n","                                 dec_padding_mask)\n","        loss = loss_function(tar_real, predictions)\n","\n","    gradients = tape.gradient(loss, transformer.trainable_variables)    \n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","  \n","    train_loss(loss)\n","    train_accuracy(tar_real, predictions)"],"execution_count":121,"outputs":[]},{"cell_type":"code","metadata":{"id":"nAjeXLMIFkH8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597142112820,"user_tz":-180,"elapsed":582116,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"9cc552cd-b087-45bc-c5ff-d3a20e3b8ce2"},"source":["def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","  \n","  # add extra dimensions to add the padding\n","  # to the attention logits.\n","    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n","\n","def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask  # (seq_len, seq_len)\n","\n","for epoch in range(100):\n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","  \n","    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n","        train_step(inp, tar)\n","        if batch % 50 == 0:\n","            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n","              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n","    \n","    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n","                                                train_loss.result(), \n","                                                train_accuracy.result()))"],"execution_count":122,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 8.4342 Accuracy 0.0000\n","Epoch 1 Batch 50 Loss 8.2729 Accuracy 0.0147\n","Epoch 1 Loss 8.2111 Accuracy 0.0269\n","Epoch 2 Batch 0 Loss 7.8675 Accuracy 0.0866\n","Epoch 2 Batch 50 Loss 7.6583 Accuracy 0.0893\n","Epoch 2 Loss 7.6211 Accuracy 0.0898\n","Epoch 3 Batch 0 Loss 7.3940 Accuracy 0.0923\n","Epoch 3 Batch 50 Loss 7.2342 Accuracy 0.0945\n","Epoch 3 Loss 7.1874 Accuracy 0.1010\n","Epoch 4 Batch 0 Loss 6.8740 Accuracy 0.1463\n","Epoch 4 Batch 50 Loss 6.5964 Accuracy 0.1779\n","Epoch 4 Loss 6.5355 Accuracy 0.1801\n","Epoch 5 Batch 0 Loss 6.1720 Accuracy 0.1925\n","Epoch 5 Batch 50 Loss 5.8858 Accuracy 0.1898\n","Epoch 5 Loss 5.8248 Accuracy 0.1899\n","Epoch 6 Batch 0 Loss 5.4360 Accuracy 0.1868\n","Epoch 6 Batch 50 Loss 5.1779 Accuracy 0.1909\n","Epoch 6 Loss 5.1201 Accuracy 0.1913\n","Epoch 7 Batch 0 Loss 4.7512 Accuracy 0.1996\n","Epoch 7 Batch 50 Loss 4.5320 Accuracy 0.1988\n","Epoch 7 Loss 4.4847 Accuracy 0.1998\n","Epoch 8 Batch 0 Loss 4.1015 Accuracy 0.2074\n","Epoch 8 Batch 50 Loss 4.0071 Accuracy 0.2084\n","Epoch 8 Loss 3.9713 Accuracy 0.2088\n","Epoch 9 Batch 0 Loss 3.7946 Accuracy 0.2081\n","Epoch 9 Batch 50 Loss 3.5917 Accuracy 0.2186\n","Epoch 9 Loss 3.5682 Accuracy 0.2200\n","Epoch 10 Batch 0 Loss 3.3689 Accuracy 0.2237\n","Epoch 10 Batch 50 Loss 3.3208 Accuracy 0.2289\n","Epoch 10 Loss 3.3008 Accuracy 0.2296\n","Epoch 11 Batch 0 Loss 3.0874 Accuracy 0.2393\n","Epoch 11 Batch 50 Loss 3.1066 Accuracy 0.2329\n","Epoch 11 Loss 3.0948 Accuracy 0.2332\n","Epoch 12 Batch 0 Loss 2.9385 Accuracy 0.2365\n","Epoch 12 Batch 50 Loss 2.9585 Accuracy 0.2372\n","Epoch 12 Loss 2.9343 Accuracy 0.2375\n","Epoch 13 Batch 0 Loss 2.8870 Accuracy 0.2379\n","Epoch 13 Batch 50 Loss 2.8029 Accuracy 0.2403\n","Epoch 13 Loss 2.8001 Accuracy 0.2407\n","Epoch 14 Batch 0 Loss 2.7963 Accuracy 0.2386\n","Epoch 14 Batch 50 Loss 2.6826 Accuracy 0.2445\n","Epoch 14 Loss 2.6786 Accuracy 0.2440\n","Epoch 15 Batch 0 Loss 2.5453 Accuracy 0.2457\n","Epoch 15 Batch 50 Loss 2.5736 Accuracy 0.2478\n","Epoch 15 Loss 2.5694 Accuracy 0.2480\n","Epoch 16 Batch 0 Loss 2.4300 Accuracy 0.2500\n","Epoch 16 Batch 50 Loss 2.4651 Accuracy 0.2513\n","Epoch 16 Loss 2.4649 Accuracy 0.2512\n","Epoch 17 Batch 0 Loss 2.2525 Accuracy 0.2521\n","Epoch 17 Batch 50 Loss 2.3615 Accuracy 0.2537\n","Epoch 17 Loss 2.3566 Accuracy 0.2543\n","Epoch 18 Batch 0 Loss 2.2990 Accuracy 0.2536\n","Epoch 18 Batch 50 Loss 2.2620 Accuracy 0.2572\n","Epoch 18 Loss 2.2555 Accuracy 0.2574\n","Epoch 19 Batch 0 Loss 2.2546 Accuracy 0.2564\n","Epoch 19 Batch 50 Loss 2.1586 Accuracy 0.2611\n","Epoch 19 Loss 2.1545 Accuracy 0.2609\n","Epoch 20 Batch 0 Loss 1.9908 Accuracy 0.2635\n","Epoch 20 Batch 50 Loss 2.0683 Accuracy 0.2627\n","Epoch 20 Loss 2.0633 Accuracy 0.2638\n","Epoch 21 Batch 0 Loss 2.0321 Accuracy 0.2656\n","Epoch 21 Batch 50 Loss 1.9693 Accuracy 0.2670\n","Epoch 21 Loss 1.9718 Accuracy 0.2667\n","Epoch 22 Batch 0 Loss 1.9080 Accuracy 0.2706\n","Epoch 22 Batch 50 Loss 1.8741 Accuracy 0.2715\n","Epoch 22 Loss 1.8712 Accuracy 0.2710\n","Epoch 23 Batch 0 Loss 1.8505 Accuracy 0.2656\n","Epoch 23 Batch 50 Loss 1.7782 Accuracy 0.2751\n","Epoch 23 Loss 1.7751 Accuracy 0.2752\n","Epoch 24 Batch 0 Loss 1.7224 Accuracy 0.2812\n","Epoch 24 Batch 50 Loss 1.6897 Accuracy 0.2789\n","Epoch 24 Loss 1.6848 Accuracy 0.2792\n","Epoch 25 Batch 0 Loss 1.6826 Accuracy 0.2741\n","Epoch 25 Batch 50 Loss 1.5938 Accuracy 0.2818\n","Epoch 25 Loss 1.5955 Accuracy 0.2819\n","Epoch 26 Batch 0 Loss 1.4896 Accuracy 0.2798\n","Epoch 26 Batch 50 Loss 1.5036 Accuracy 0.2873\n","Epoch 26 Loss 1.5081 Accuracy 0.2868\n","Epoch 27 Batch 0 Loss 1.3249 Accuracy 0.3011\n","Epoch 27 Batch 50 Loss 1.4144 Accuracy 0.2916\n","Epoch 27 Loss 1.4222 Accuracy 0.2910\n","Epoch 28 Batch 0 Loss 1.2932 Accuracy 0.2997\n","Epoch 28 Batch 50 Loss 1.3365 Accuracy 0.2952\n","Epoch 28 Loss 1.3382 Accuracy 0.2952\n","Epoch 29 Batch 0 Loss 1.2140 Accuracy 0.3054\n","Epoch 29 Batch 50 Loss 1.2451 Accuracy 0.3007\n","Epoch 29 Loss 1.2482 Accuracy 0.3007\n","Epoch 30 Batch 0 Loss 1.1034 Accuracy 0.3146\n","Epoch 30 Batch 50 Loss 1.1730 Accuracy 0.3045\n","Epoch 30 Loss 1.1735 Accuracy 0.3045\n","Epoch 31 Batch 0 Loss 1.0662 Accuracy 0.3082\n","Epoch 31 Batch 50 Loss 1.0893 Accuracy 0.3095\n","Epoch 31 Loss 1.0950 Accuracy 0.3087\n","Epoch 32 Batch 0 Loss 0.8998 Accuracy 0.3288\n","Epoch 32 Batch 50 Loss 1.0100 Accuracy 0.3151\n","Epoch 32 Loss 1.0188 Accuracy 0.3135\n","Epoch 33 Batch 0 Loss 0.9450 Accuracy 0.3232\n","Epoch 33 Batch 50 Loss 0.9343 Accuracy 0.3196\n","Epoch 33 Loss 0.9442 Accuracy 0.3182\n","Epoch 34 Batch 0 Loss 0.8201 Accuracy 0.3281\n","Epoch 34 Batch 50 Loss 0.8760 Accuracy 0.3216\n","Epoch 34 Loss 0.8860 Accuracy 0.3204\n","Epoch 35 Batch 0 Loss 0.7858 Accuracy 0.3295\n","Epoch 35 Batch 50 Loss 0.8175 Accuracy 0.3256\n","Epoch 35 Loss 0.8248 Accuracy 0.3241\n","Epoch 36 Batch 0 Loss 0.7276 Accuracy 0.3352\n","Epoch 36 Batch 50 Loss 0.7598 Accuracy 0.3291\n","Epoch 36 Loss 0.7693 Accuracy 0.3279\n","Epoch 37 Batch 0 Loss 0.6525 Accuracy 0.3381\n","Epoch 37 Batch 50 Loss 0.7075 Accuracy 0.3329\n","Epoch 37 Loss 0.7187 Accuracy 0.3313\n","Epoch 38 Batch 0 Loss 0.6446 Accuracy 0.3416\n","Epoch 38 Batch 50 Loss 0.6581 Accuracy 0.3361\n","Epoch 38 Loss 0.6667 Accuracy 0.3343\n","Epoch 39 Batch 0 Loss 0.5445 Accuracy 0.3551\n","Epoch 39 Batch 50 Loss 0.6232 Accuracy 0.3381\n","Epoch 39 Loss 0.6333 Accuracy 0.3363\n","Epoch 40 Batch 0 Loss 0.5380 Accuracy 0.3480\n","Epoch 40 Batch 50 Loss 0.5749 Accuracy 0.3411\n","Epoch 40 Loss 0.5910 Accuracy 0.3385\n","Epoch 41 Batch 0 Loss 0.5205 Accuracy 0.3473\n","Epoch 41 Batch 50 Loss 0.5520 Accuracy 0.3413\n","Epoch 41 Loss 0.5581 Accuracy 0.3405\n","Epoch 42 Batch 0 Loss 0.4690 Accuracy 0.3516\n","Epoch 42 Batch 50 Loss 0.5237 Accuracy 0.3428\n","Epoch 42 Loss 0.5361 Accuracy 0.3416\n","Epoch 43 Batch 0 Loss 0.4502 Accuracy 0.3658\n","Epoch 43 Batch 50 Loss 0.5036 Accuracy 0.3449\n","Epoch 43 Loss 0.5154 Accuracy 0.3426\n","Epoch 44 Batch 0 Loss 0.4601 Accuracy 0.3494\n","Epoch 44 Batch 50 Loss 0.4939 Accuracy 0.3424\n","Epoch 44 Loss 0.5033 Accuracy 0.3415\n","Epoch 45 Batch 0 Loss 0.4339 Accuracy 0.3501\n","Epoch 45 Batch 50 Loss 0.4664 Accuracy 0.3462\n","Epoch 45 Loss 0.4775 Accuracy 0.3444\n","Epoch 46 Batch 0 Loss 0.3991 Accuracy 0.3679\n","Epoch 46 Batch 50 Loss 0.4542 Accuracy 0.3467\n","Epoch 46 Loss 0.4689 Accuracy 0.3446\n","Epoch 47 Batch 0 Loss 0.3728 Accuracy 0.3565\n","Epoch 47 Batch 50 Loss 0.4448 Accuracy 0.3469\n","Epoch 47 Loss 0.4574 Accuracy 0.3451\n","Epoch 48 Batch 0 Loss 0.3981 Accuracy 0.3565\n","Epoch 48 Batch 50 Loss 0.4439 Accuracy 0.3465\n","Epoch 48 Loss 0.4531 Accuracy 0.3454\n","Epoch 49 Batch 0 Loss 0.3563 Accuracy 0.3608\n","Epoch 49 Batch 50 Loss 0.4412 Accuracy 0.3459\n","Epoch 49 Loss 0.4542 Accuracy 0.3440\n","Epoch 50 Batch 0 Loss 0.3864 Accuracy 0.3452\n","Epoch 50 Batch 50 Loss 0.4287 Accuracy 0.3463\n","Epoch 50 Loss 0.4431 Accuracy 0.3450\n","Epoch 51 Batch 0 Loss 0.4093 Accuracy 0.3643\n","Epoch 51 Batch 50 Loss 0.4106 Accuracy 0.3488\n","Epoch 51 Loss 0.4270 Accuracy 0.3468\n","Epoch 52 Batch 0 Loss 0.3155 Accuracy 0.3700\n","Epoch 52 Batch 50 Loss 0.4225 Accuracy 0.3470\n","Epoch 52 Loss 0.4361 Accuracy 0.3455\n","Epoch 53 Batch 0 Loss 0.4064 Accuracy 0.3501\n","Epoch 53 Batch 50 Loss 0.4238 Accuracy 0.3467\n","Epoch 53 Loss 0.4361 Accuracy 0.3455\n","Epoch 54 Batch 0 Loss 0.3935 Accuracy 0.3523\n","Epoch 54 Batch 50 Loss 0.4129 Accuracy 0.3488\n","Epoch 54 Loss 0.4267 Accuracy 0.3459\n","Epoch 55 Batch 0 Loss 0.3772 Accuracy 0.3494\n","Epoch 55 Batch 50 Loss 0.4179 Accuracy 0.3470\n","Epoch 55 Loss 0.4310 Accuracy 0.3453\n","Epoch 56 Batch 0 Loss 0.3455 Accuracy 0.3743\n","Epoch 56 Batch 50 Loss 0.4092 Accuracy 0.3473\n","Epoch 56 Loss 0.4207 Accuracy 0.3465\n","Epoch 57 Batch 0 Loss 0.2560 Accuracy 0.3594\n","Epoch 57 Batch 50 Loss 0.4149 Accuracy 0.3473\n","Epoch 57 Loss 0.4292 Accuracy 0.3456\n","Epoch 58 Batch 0 Loss 0.3960 Accuracy 0.3480\n","Epoch 58 Batch 50 Loss 0.4036 Accuracy 0.3488\n","Epoch 58 Loss 0.4173 Accuracy 0.3467\n","Epoch 59 Batch 0 Loss 0.3422 Accuracy 0.3672\n","Epoch 59 Batch 50 Loss 0.4091 Accuracy 0.3493\n","Epoch 59 Loss 0.4229 Accuracy 0.3470\n","Epoch 60 Batch 0 Loss 0.3116 Accuracy 0.3665\n","Epoch 60 Batch 50 Loss 0.4127 Accuracy 0.3478\n","Epoch 60 Loss 0.4262 Accuracy 0.3464\n","Epoch 61 Batch 0 Loss 0.4015 Accuracy 0.3359\n","Epoch 61 Batch 50 Loss 0.4098 Accuracy 0.3479\n","Epoch 61 Loss 0.4215 Accuracy 0.3464\n","Epoch 62 Batch 0 Loss 0.3962 Accuracy 0.3494\n","Epoch 62 Batch 50 Loss 0.4129 Accuracy 0.3484\n","Epoch 62 Loss 0.4236 Accuracy 0.3468\n","Epoch 63 Batch 0 Loss 0.3285 Accuracy 0.3565\n","Epoch 63 Batch 50 Loss 0.4108 Accuracy 0.3491\n","Epoch 63 Loss 0.4258 Accuracy 0.3473\n","Epoch 64 Batch 0 Loss 0.3179 Accuracy 0.3601\n","Epoch 64 Batch 50 Loss 0.4035 Accuracy 0.3486\n","Epoch 64 Loss 0.4185 Accuracy 0.3472\n","Epoch 65 Batch 0 Loss 0.2984 Accuracy 0.3551\n","Epoch 65 Batch 50 Loss 0.4112 Accuracy 0.3488\n","Epoch 65 Loss 0.4250 Accuracy 0.3474\n","Epoch 66 Batch 0 Loss 0.3586 Accuracy 0.3501\n","Epoch 66 Batch 50 Loss 0.4105 Accuracy 0.3482\n","Epoch 66 Loss 0.4187 Accuracy 0.3474\n","Epoch 67 Batch 0 Loss 0.3080 Accuracy 0.3672\n","Epoch 67 Batch 50 Loss 0.4001 Accuracy 0.3492\n","Epoch 67 Loss 0.4157 Accuracy 0.3473\n","Epoch 68 Batch 0 Loss 0.2844 Accuracy 0.3757\n","Epoch 68 Batch 50 Loss 0.3923 Accuracy 0.3496\n","Epoch 68 Loss 0.4042 Accuracy 0.3488\n","Epoch 69 Batch 0 Loss 0.3448 Accuracy 0.3501\n","Epoch 69 Batch 50 Loss 0.3872 Accuracy 0.3509\n","Epoch 69 Loss 0.3984 Accuracy 0.3491\n","Epoch 70 Batch 0 Loss 0.3517 Accuracy 0.3693\n","Epoch 70 Batch 50 Loss 0.3851 Accuracy 0.3503\n","Epoch 70 Loss 0.3983 Accuracy 0.3491\n","Epoch 71 Batch 0 Loss 0.3142 Accuracy 0.3601\n","Epoch 71 Batch 50 Loss 0.3813 Accuracy 0.3518\n","Epoch 71 Loss 0.3941 Accuracy 0.3501\n","Epoch 72 Batch 0 Loss 0.3478 Accuracy 0.3587\n","Epoch 72 Batch 50 Loss 0.3782 Accuracy 0.3511\n","Epoch 72 Loss 0.3906 Accuracy 0.3489\n","Epoch 73 Batch 0 Loss 0.2913 Accuracy 0.3494\n","Epoch 73 Batch 50 Loss 0.3719 Accuracy 0.3527\n","Epoch 73 Loss 0.3857 Accuracy 0.3509\n","Epoch 74 Batch 0 Loss 0.3678 Accuracy 0.3672\n","Epoch 74 Batch 50 Loss 0.3719 Accuracy 0.3513\n","Epoch 74 Loss 0.3848 Accuracy 0.3498\n","Epoch 75 Batch 0 Loss 0.3169 Accuracy 0.3544\n","Epoch 75 Batch 50 Loss 0.3713 Accuracy 0.3524\n","Epoch 75 Loss 0.3829 Accuracy 0.3505\n","Epoch 76 Batch 0 Loss 0.3399 Accuracy 0.3572\n","Epoch 76 Batch 50 Loss 0.3633 Accuracy 0.3529\n","Epoch 76 Loss 0.3798 Accuracy 0.3508\n","Epoch 77 Batch 0 Loss 0.3117 Accuracy 0.3622\n","Epoch 77 Batch 50 Loss 0.3622 Accuracy 0.3523\n","Epoch 77 Loss 0.3734 Accuracy 0.3511\n","Epoch 78 Batch 0 Loss 0.3130 Accuracy 0.3658\n","Epoch 78 Batch 50 Loss 0.3579 Accuracy 0.3530\n","Epoch 78 Loss 0.3739 Accuracy 0.3514\n","Epoch 79 Batch 0 Loss 0.2903 Accuracy 0.3636\n","Epoch 79 Batch 50 Loss 0.3545 Accuracy 0.3536\n","Epoch 79 Loss 0.3685 Accuracy 0.3519\n","Epoch 80 Batch 0 Loss 0.3086 Accuracy 0.3587\n","Epoch 80 Batch 50 Loss 0.3492 Accuracy 0.3538\n","Epoch 80 Loss 0.3609 Accuracy 0.3523\n","Epoch 81 Batch 0 Loss 0.2752 Accuracy 0.3594\n","Epoch 81 Batch 50 Loss 0.3534 Accuracy 0.3526\n","Epoch 81 Loss 0.3636 Accuracy 0.3515\n","Epoch 82 Batch 0 Loss 0.2351 Accuracy 0.3714\n","Epoch 82 Batch 50 Loss 0.3444 Accuracy 0.3534\n","Epoch 82 Loss 0.3608 Accuracy 0.3520\n","Epoch 83 Batch 0 Loss 0.2618 Accuracy 0.3686\n","Epoch 83 Batch 50 Loss 0.3464 Accuracy 0.3541\n","Epoch 83 Loss 0.3591 Accuracy 0.3523\n","Epoch 84 Batch 0 Loss 0.2910 Accuracy 0.3572\n","Epoch 84 Batch 50 Loss 0.3421 Accuracy 0.3542\n","Epoch 84 Loss 0.3563 Accuracy 0.3525\n","Epoch 85 Batch 0 Loss 0.2782 Accuracy 0.3700\n","Epoch 85 Batch 50 Loss 0.3404 Accuracy 0.3542\n","Epoch 85 Loss 0.3536 Accuracy 0.3523\n","Epoch 86 Batch 0 Loss 0.3024 Accuracy 0.3445\n","Epoch 86 Batch 50 Loss 0.3375 Accuracy 0.3533\n","Epoch 86 Loss 0.3501 Accuracy 0.3525\n","Epoch 87 Batch 0 Loss 0.2670 Accuracy 0.3714\n","Epoch 87 Batch 50 Loss 0.3379 Accuracy 0.3547\n","Epoch 87 Loss 0.3513 Accuracy 0.3530\n","Epoch 88 Batch 0 Loss 0.2837 Accuracy 0.3594\n","Epoch 88 Batch 50 Loss 0.3339 Accuracy 0.3544\n","Epoch 88 Loss 0.3462 Accuracy 0.3528\n","Epoch 89 Batch 0 Loss 0.2635 Accuracy 0.3601\n","Epoch 89 Batch 50 Loss 0.3273 Accuracy 0.3547\n","Epoch 89 Loss 0.3418 Accuracy 0.3530\n","Epoch 90 Batch 0 Loss 0.2215 Accuracy 0.3828\n","Epoch 90 Batch 50 Loss 0.3263 Accuracy 0.3554\n","Epoch 90 Loss 0.3408 Accuracy 0.3532\n","Epoch 91 Batch 0 Loss 0.2323 Accuracy 0.3622\n","Epoch 91 Batch 50 Loss 0.3255 Accuracy 0.3559\n","Epoch 91 Loss 0.3368 Accuracy 0.3538\n","Epoch 92 Batch 0 Loss 0.2115 Accuracy 0.3793\n","Epoch 92 Batch 50 Loss 0.3274 Accuracy 0.3560\n","Epoch 92 Loss 0.3374 Accuracy 0.3539\n","Epoch 93 Batch 0 Loss 0.2767 Accuracy 0.3494\n","Epoch 93 Batch 50 Loss 0.3249 Accuracy 0.3550\n","Epoch 93 Loss 0.3349 Accuracy 0.3543\n","Epoch 94 Batch 0 Loss 0.3115 Accuracy 0.3572\n","Epoch 94 Batch 50 Loss 0.3267 Accuracy 0.3545\n","Epoch 94 Loss 0.3364 Accuracy 0.3537\n","Epoch 95 Batch 0 Loss 0.2892 Accuracy 0.3580\n","Epoch 95 Batch 50 Loss 0.3185 Accuracy 0.3550\n","Epoch 95 Loss 0.3324 Accuracy 0.3535\n","Epoch 96 Batch 0 Loss 0.2717 Accuracy 0.3594\n","Epoch 96 Batch 50 Loss 0.3217 Accuracy 0.3539\n","Epoch 96 Loss 0.3330 Accuracy 0.3530\n","Epoch 97 Batch 0 Loss 0.2639 Accuracy 0.3615\n","Epoch 97 Batch 50 Loss 0.3191 Accuracy 0.3556\n","Epoch 97 Loss 0.3305 Accuracy 0.3544\n","Epoch 98 Batch 0 Loss 0.2839 Accuracy 0.3544\n","Epoch 98 Batch 50 Loss 0.3142 Accuracy 0.3558\n","Epoch 98 Loss 0.3288 Accuracy 0.3542\n","Epoch 99 Batch 0 Loss 0.2541 Accuracy 0.3651\n","Epoch 99 Batch 50 Loss 0.3120 Accuracy 0.3558\n","Epoch 99 Loss 0.3231 Accuracy 0.3542\n","Epoch 100 Batch 0 Loss 0.2466 Accuracy 0.3700\n","Epoch 100 Batch 50 Loss 0.3114 Accuracy 0.3557\n","Epoch 100 Loss 0.3210 Accuracy 0.3543\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hwlC8k9_FkH_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597142112824,"user_tz":-180,"elapsed":581337,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"34e8aa65-235d-4c92-fbd3-ffea9267da47"},"source":["def evaluate(inp_sentence):\n","    start_token = [1]\n","    end_token = [2]\n","  \n","    sentence = preprocess_sentence(inp_sentence)\n","    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n","    \n","    encoder_input = tf.expand_dims(inputs, 0)\n","  \n","  # as the target is english, the first word to the transformer should be the\n","  # english start token.\n","    decoder_input = [1]\n","    output = tf.expand_dims(decoder_input, 0)\n","    \n","    for i in range(max_length_targ):\n","        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","            encoder_input, output)\n","  \n","    # predictions.shape == (batch_size, seq_len, vocab_size)\n","        predictions, attention_weights = transformer(encoder_input, \n","                                                 output,\n","                                                 False,\n","                                                 enc_padding_mask,\n","                                                 combined_mask,\n","                                                 dec_padding_mask)\n","    \n","    # select the last word from the seq_len dimension\n","        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","    \n","    # return the result if the predicted_id is equal to the end token\n","        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n","            return tf.squeeze(output, axis=0), attention_weights\n","    \n","    # concatentate the predicted_id to the output which is given to the decoder\n","    # as its input.\n","        output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights\n","\n","\n","def plot_attention_weights(attention, sentence, result, layer):\n","    fig = plt.figure(figsize=(16, 8))\n","  \n","    sentence = inp_lang_tokenizer.encode(sentence)\n","  \n","    attention = tf.squeeze(attention[layer], axis=0)\n","  \n","    for head in range(attention.shape[0]):\n","        ax = fig.add_subplot(2, 4, head+1)\n","\n","        # plot the attention weights\n","        ax.matshow(attention[head][:-1, :], cmap='viridis')\n","\n","        fontdict = {'fontsize': 10}\n","\n","        ax.set_xticks(range(len(sentence)+2))\n","        ax.set_yticks(range(len(result)))\n","\n","        ax.set_ylim(len(result)-1.5, -0.5)\n","\n","        ax.set_xticklabels(\n","            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n","            fontdict=fontdict, rotation=90)\n","\n","        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n","                            if i < tokenizer_en.vocab_size], \n","                           fontdict=fontdict)\n","\n","        ax.set_xlabel('Head {}'.format(head+1))\n","  \n","    plt.tight_layout()\n","    plt.show()\n","\n","def translate(sentence, plot=''):\n","    result, attention_weights = evaluate(sentence)\n","    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n","\n","    print('Input: {}'.format(sentence))\n","    print('Predicted translation: {}'.format(predicted_sentence))\n","  \n","    if plot:\n","        plot_attention_weights(attention_weights, sentence, result, plot)\n","        \n","translate(\"good morning.\")"],"execution_count":123,"outputs":[{"output_type":"stream","text":["Input: good morning.\n","Predicted translation: ['<start>', 'доброе', 'утро', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wRUjINBG9-Ru","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597142622417,"user_tz":-180,"elapsed":1181,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"bbdb7bf0-445b-4bc8-e9ea-9d5c4d5f1426"},"source":["translate(\"got it.\")"],"execution_count":125,"outputs":[{"output_type":"stream","text":["Input: got it.\n","Predicted translation: ['<start>', 'понял', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T8FfTZv2Hmw2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597142648010,"user_tz":-180,"elapsed":1087,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"d9e5c6b0-940e-4d86-c0f9-6224e2f87d2d"},"source":["translate(\"got it?\")"],"execution_count":127,"outputs":[{"output_type":"stream","text":["Input: got it?\n","Predicted translation: ['<start>', 'усекла', '?']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SzLLo63JHlu-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597142669770,"user_tz":-180,"elapsed":1157,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"6336bc41-5d8d-4a47-9571-aeabd9f0bd10"},"source":["translate(\"got it!\")"],"execution_count":128,"outputs":[{"output_type":"stream","text":["Input: got it!\n","Predicted translation: ['<start>', 'понял', '!']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1I-FZadaHxTq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597142696045,"user_tz":-180,"elapsed":1272,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"c30a9dc6-1f81-408c-e1f9-c4a66883c170"},"source":["translate(\"will you go?\")"],"execution_count":130,"outputs":[{"output_type":"stream","text":["Input: will you go?\n","Predicted translation: ['<start>', 'пойдёте', '?']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q2kR4trbH0ba","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597142708083,"user_tz":-180,"elapsed":1196,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"4724db56-0cfe-45c7-b641-673cd8af67f0"},"source":["translate(\"did you go?\")"],"execution_count":131,"outputs":[{"output_type":"stream","text":["Input: did you go?\n","Predicted translation: ['<start>', 'вы', 'умираете', '?']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZKE_u6l1H6pf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597142755723,"user_tz":-180,"elapsed":1276,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"14ab7852-bec2-413b-90fe-e153ad844250"},"source":["translate(\"i try\")"],"execution_count":133,"outputs":[{"output_type":"stream","text":["Input: i try\n","Predicted translation: ['<start>', 'я', 'пробую', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uF8B_FUuIBL4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597142769461,"user_tz":-180,"elapsed":1369,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"29df3c42-7c1d-439c-a93a-04bad9bd649d"},"source":["translate(\"i see\")"],"execution_count":134,"outputs":[{"output_type":"stream","text":["Input: i see\n","Predicted translation: ['<start>', 'понимаю', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YDBu2qKtIJlv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1597142784075,"user_tz":-180,"elapsed":1158,"user":{"displayName":"Aleksey Dorkin","photoUrl":"","userId":"13020610572226923744"}},"outputId":"22a6e9a1-0d65-4809-9b91-f1fefbf1ec9d"},"source":["translate(\"i see you\")"],"execution_count":135,"outputs":[{"output_type":"stream","text":["Input: i see you\n","Predicted translation: ['<start>', 'я', 'тебя', 'вижу', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CZEpnJmKITPr","colab_type":"text"},"source":[" В целом выглядит наиболее вменяемо, но и здесь странности встречаются."]},{"cell_type":"code","metadata":{"id":"21kQwnLdKEIe","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}