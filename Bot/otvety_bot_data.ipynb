{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from stop_words import get_stop_words\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import notebook\n",
    "import json\n",
    "import pymorphy2\n",
    "import hnswlib\n",
    "from string import ascii_letters\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы определить наиболее близкий вопрос к заданному полезны могут быть слова, написанные кириллицей и латиницей. Пунктуация в данном датасете обширно используется дя придания экспрессивной окраски, но на содержательную составляющую влияет слабо. Числа могли бы быть полезны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых текстах попадаются html теги, которые целесообразно удалить (тематика вопросов всё-таки не такая узкая)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_re = re.compile(r'<[^>]+>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_re = re.compile(rf'[А-Яа-яЁёA-Za-z]+|[0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(token_re, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = tag_re.sub('', text)\n",
    "    tokens = tokenize(text)\n",
    "    return [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 200\n",
    "MAX_LINES = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path, limit=0):\n",
    "    sentences = []\n",
    "    count = 0\n",
    "    with open(path, 'r') as file:\n",
    "        for line in notebook.tqdm(file):\n",
    "            sentence = clean(line)\n",
    "            count += 1\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "            if limit and count > limit:\n",
    "                break\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = tag_re.sub('', text)\n",
    "    tokens = tokenize(text)\n",
    "    \n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sent(tokens, model):\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    n_tokens = len(tokens)\n",
    "    if not n_tokens:\n",
    "        return vector\n",
    "    for token in tokens:\n",
    "        vector += model.wv.get_vector(token)\n",
    "    vector /= n_tokens\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tokens):\n",
    "    return [morph.parse(token.lower())[0].normal_form for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(get_stop_words(\"ru\")).union(set(('стать', 'иметь', 'быть')))\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510c24679a584018ac7dec8cf1d7d62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = read_corpus('/mnt/f/data/answers.txt', MAX_LINES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affe667701ad4127ba3f1d9a10e963d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=84779.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = []\n",
    "for sentence in notebook.tqdm(sentences):\n",
    "    tmp.append(normalize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.8 s, sys: 594 ms, total: 14.3 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "phrases = Phrases(sentences,\n",
    "                  common_terms=list(stop_words),\n",
    "                  threshold=10,\n",
    "                  min_count=5)\n",
    "bigram = Phraser(phrases)\n",
    "tokens = list(bigram[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [remove_stop_words(sent) for sent in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.8 s, sys: 78.1 ms, total: 2.88 s\n",
      "Wall time: 2.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ft = FastText(sentences=sentences,\n",
    "              size=EMBEDDING_SIZE,\n",
    "              min_count=10,\n",
    "              window=3,\n",
    "              workers=-1,\n",
    "              max_vocab_size=30_000,\n",
    "              negative=10,\n",
    "              bucket=1000\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hnswlib.Index(space ='cosine', dim=EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.init_index(max_elements=len(tokens), ef_construction=200, M=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_sents = np.vstack([vectorize_sent(sentence, ft) for sentence in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84779, 200)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_sents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.add_items(v_sents, list(range(len(tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, distances = p.knn_query(v_sents[600], k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  600, 18621, 18628]], dtype=uint64)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.5762787e-07, -3.5762787e-07, -3.5762787e-07]], dtype=float32)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['огонь']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
