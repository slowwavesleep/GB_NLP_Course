{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from stop_words import get_stop_words\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import notebook\n",
    "import json\n",
    "import pymorphy2\n",
    "import hnswlib\n",
    "from string import ascii_letters\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы определить наиболее близкий вопрос к заданному полезны могут быть слова, написанные кириллицей и латиницей. Пунктуация в данном датасете обширно используется дя придания экспрессивной окраски, но на содержательную составляющую влияет слабо. Числа могли бы быть полезны, но получаемые результаты не оправдывают их использование."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых текстах попадаются html теги, которые целесообразно удалить (тематика вопросов всё-таки не такая узкая)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_re = re.compile(r'<[^>]+>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_re = re.compile(rf'[А-Яа-яЁёA-Za-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(token_re, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = tag_re.sub('', text)\n",
    "    tokens = tokenize(text)\n",
    "    return [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 100\n",
    "MAX_LINES = 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим вопросы и ответы при чтении корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path, limit=0):\n",
    "    questions = []\n",
    "    answers = defaultdict(lambda: [])\n",
    "    count = 0\n",
    "    question_index = -1\n",
    "    new_question = False\n",
    "    end = False\n",
    "    with open(path, 'r') as file:\n",
    "        for line in notebook.tqdm(file):\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if line == '\\n':\n",
    "                continue\n",
    "                \n",
    "            if limit and count > limit:\n",
    "                end = True\n",
    "            \n",
    "\n",
    "            if line == '---\\n':\n",
    "                new_question = True\n",
    "                if end:\n",
    "                    break\n",
    "            elif new_question:\n",
    "                questions.append(clean(line))\n",
    "                new_question = False\n",
    "                question_index += 1\n",
    "            else:\n",
    "                answers[question_index].append(line)\n",
    "                \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = tag_re.sub('', text)\n",
    "    tokens = tokenize(text)\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tokens):\n",
    "    return [morph.parse(token.lower())[0].normal_form for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополним стоп-слова глаголами, которые для данной задачи не очень помогают в определении содержания вопросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(get_stop_words(\"ru\")).union(set(('стать', 'иметь', 'быть', 'являться')))\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e14a9af8bd47cb833567eebae423e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions, answers = read_corpus('/mnt/f/data/answers.txt', MAX_LINES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые вопросы просто огромные. Есть смысл их обрезать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "904"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokens) for tokens in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.58840011388932"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(tokens) for tokens in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median([len(tokens) for tokens in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [tokens[:min(50, len(tokens) - 1)] for tokens in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной концепции есть смысл обучать векторизатор только на вопросах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d117c7f051734650afc3c062f1e65228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=154536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions_norm = []\n",
    "for sentence in notebook.tqdm(questions):\n",
    "    questions_norm.append(normalize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.8 s, sys: 859 ms, total: 14.7 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "phrases = Phrases(questions_norm,\n",
    "                  common_terms=list(stop_words),\n",
    "                  threshold=10,\n",
    "                  min_count=10)\n",
    "bigram = Phraser(phrases)\n",
    "questions_norm_ph = list(bigram[questions_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_norm_ph = [remove_stop_words(sent) for sent in questions_norm_ph]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно взвесим вектора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.8 s, sys: 125 ms, total: 2.92 s\n",
      "Wall time: 2.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ft = FastText(sentences=questions_norm_ph,\n",
    "              size=EMBEDDING_SIZE,\n",
    "              min_count=20,\n",
    "              window=3,\n",
    "              workers=-1,\n",
    "              max_vocab_size=30_000,\n",
    "              negative=10,\n",
    "              bucket=1000\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.save('/mnt/f/data/bot/ft.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_norm_ph_str = [' '.join(tokens) for tokens in questions_norm_ph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect = TfidfVectorizer(stop_words=None)\n",
    "final_tf_idf = tf_idf_vect.fit_transform(questions_norm_ph_str)\n",
    "tfidf_feat = tf_idf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/f/data/bot/tfidf.pkl', 'wb') as file:\n",
    "    pickle.dump(tf_idf_vect, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sent(tokens, sentence_num, model):\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    n_tokens = len(tokens)\n",
    "    weight_sum = 0\n",
    "    \n",
    "    if not n_tokens:\n",
    "        return vector\n",
    "    \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            weight = final_tf_idf[sentence_num, tfidf_feat.index(token)]\n",
    "        except:\n",
    "            weight = 0\n",
    "        vector += (model.wv.get_vector(token) * weight)\n",
    "        weight_sum += weight\n",
    "        \n",
    "    if not weight_sum:\n",
    "        return vector * 0\n",
    "        \n",
    "    vector /= weight_sum\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154536"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions_norm_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302213a05b074173884a3d0254f414e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions_vect = []\n",
    "for num, sentence in notebook.tqdm(enumerate(questions_norm_ph)):\n",
    "    questions_vect.append(vectorize_sent(sentence, num, ft))\n",
    "                          \n",
    "questions_vect = np.vstack(questions_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hnswlib.Index(space ='cosine', dim=EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.init_index(max_elements=len(questions_norm_ph), ef_construction=200, M=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.add_items(questions_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.save_index('/mnt/f/data/bot/index.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/f/data/bot/answers.json', 'w') as file:\n",
    "    json.dump(dict(answers), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, distances = p.knn_query(questions_vect[300], k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  300, 72121, 67385]], dtype=uint64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['тухлый', 'аж', 'задать_вопрос']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_norm_ph[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Конечно!!! На панели ?. \\n',\n",
       " 'нет..тебя не хватит.а хотя может ты такая влюбвиобильная. \\n',\n",
       " 'Я не думаю что ты влюбилась в них троих, просто ты испытуеш к ним большую симпатию. Определись кто тебе больше нравится, знаешь одной попой на 2 (в твоём случае 3) стула не сядишь.. \\n',\n",
       " 'Устрой одновременно свидание со всеми троими и посмотри, что из этого выйдет:)<br>А зачем ты всех зомбируешь своей рассылкой? Сначала идет игра, а в конце угроза: не отправишь - 10 лет мучаться будешь... Ты сама-то в эту хрень веришь?. \\n']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[72121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
